{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMKlxriHlnZTEZkOkZFMZ3+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashaduzzaman-sarker/Image-Classification/blob/main/ShiftViT_A_Vision_Transformer_without_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ShiftViT_A Vision Transformer without Attention\n",
        "\n",
        "**Author:** [Ashaduzzaman Sarker](https://github.com/ashaduzzaman-sarker/)\n",
        "<br>\n",
        "**Date created:** 30/06/2024\n",
        "**Reference:**\n",
        "\n",
        " - ShiftViT : [When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism](\n",
        "https://doi.org/10.48550/arXiv.2201.10801)\n",
        "\n",
        " - [Keras](https://keras.io/examples/vision/shiftvit/)"
      ],
      "metadata": {
        "id": "oecMUBqezyo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        "  - Vision Transformers (ViTs) have sparked a wave of research at the intersection of Transformers and Computer Vision (CV).\n",
        "  - ViTs can simultaneously model long- and short-range dependencies, thanks to the Multi-Head Self-Attention mechanism in the Transformer block.\n",
        "  - Many researchers believe that the success of ViTs is purely due to the attention layer, and they seldom consider other parts of the ViT model.\n",
        "  - In the academic paper \"When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism,\" the authors propose demystifying the success of ViTs by introducing a no-parameter operation in place of the attention operation.\n",
        "  - They replace the attention operation with a shifting operation.\n",
        "  - This example minimally implements the paper, closely aligning with the author's official implementation.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "- **Vision Transformers (ViTs) and Their Impact**:\n",
        "  - The introduction of ViTs has created significant interest and research in the overlap between Transformers, traditionally used in Natural Language Processing (NLP), and Computer Vision (CV).\n",
        "  \n",
        "- **Capabilities of ViTs**:\n",
        "  - ViTs are notable for their ability to model dependencies at varying scales within an image, from global (long-range) to local (short-range), using the Multi-Head Self-Attention mechanism.\n",
        "  \n",
        "- **Common Beliefs**:\n",
        "  - The success of ViTs is often attributed solely to the attention mechanism. Other components of the ViT architecture are frequently overlooked.\n",
        "  \n",
        "- **New Perspective**:\n",
        "  - The paper \"When Shift Operation Meets Vision Transformer\" challenges the conventional view by suggesting that the attention mechanism might not be the only reason for ViTs' success.\n",
        "  \n",
        "- **Proposed Alternative**:\n",
        "  - The authors propose an alternative to the attention mechanism—a shifting operation that does not require parameters—potentially simplifying the model.\n",
        "  \n",
        "- **Implementation**:\n",
        "  - The example provided aims to implement the concepts from the paper in a way that is faithful to the authors' original implementation, allowing for a practical exploration of this new approach.\n",
        "\n",
        "  ![](https://pbs.twimg.com/media/FKE40NAWYAE_lMO.jpg)"
      ],
      "metadata": {
        "id": "pmptDYWO1sbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and imports"
      ],
      "metadata": {
        "id": "IWNDSfwv2EPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq7roxj_3C8P",
        "outputId": "0ca53974-e117-4856-8a57-9587fde6a42e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/611.8 kB\u001b[0m \u001b[31m805.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m604.2/611.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade keras"
      ],
      "metadata": {
        "id": "vUJ_1vc9TI_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz8yPoYOyw3u",
        "outputId": "58a9fc3e-9081-4a4b-b313-24062dfc0c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import pathlib\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "keras.utils.set_random_seed(SEED)"
      ],
      "metadata": {
        "id": "-jUXywyD3Rip"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "Kj7WI6j63fTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "  # DATA\n",
        "  batch_size = 256\n",
        "  buffer_size = batch_size * 2\n",
        "  input_shape = (32, 32, 3)\n",
        "  num_classes = 10\n",
        "\n",
        "  # AUGMENTATION\n",
        "  image_size = 48\n",
        "\n",
        "  # ARCHIECTURE\n",
        "  patch_size = 4\n",
        "  projection_dim = 96\n",
        "  num_shift_blocks_per_stages = [2, 4, 8, 2]\n",
        "  epsilon = 1e-5\n",
        "  stochastic_depth_rate = 0.2\n",
        "  mlp_dropout_rate = 0.2\n",
        "  num_div = 12\n",
        "  shift_pixel = 1\n",
        "  mlp_expand_ratio = 2\n",
        "\n",
        "  # OPTIMIZER\n",
        "  lr_start = 1e-5\n",
        "  lr_max = 1e-3\n",
        "  weight_decay = 1e-4\n",
        "\n",
        "  # TRAINING\n",
        "  epochs = 5 #100\n",
        "\n",
        "  # INFERENCE\n",
        "  label_map = {\n",
        "      0: 'airplane',\n",
        "      1: 'automobile',\n",
        "      2: 'bird',\n",
        "      3: 'cat',\n",
        "      4: 'deer',\n",
        "      5: 'dog',\n",
        "      6: 'frog',\n",
        "      7: 'horse',\n",
        "      8: 'ship',\n",
        "      9: 'truck',\n",
        "  }\n",
        "  tf_ds_batch_size = 20\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "0uTKAWNK3j3v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the CIFAR-10 dataset"
      ],
      "metadata": {
        "id": "IjBUWArC5EYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvw1iHbR5B0R",
        "outputId": "49f1ca38-5759-40e4-937c-270b07967332"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 10s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_val, y_val) = (\n",
        "    (x_train[:40000], y_train[:40000]),\n",
        "    (x_train[40000:], y_train[40000:]),\n",
        ")\n",
        "\n",
        "print(f'Training samples: {len(x_train)}')\n",
        "print(f'Validation samples: {len(x_val)}')\n",
        "print(f'Testing samples: {len(x_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMcqgX485Q9q",
        "outputId": "4ab8ca94-93d2-44f0-b137-0ff103477404"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 40000\n",
            "Validation samples: 10000\n",
            "Testing samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = train_ds.shuffle(config.buffer_size).batch(config.batch_size).prefetch(AUTO)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_ds = val_ds.batch(config.batch_size).prefetch(AUTO)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_ds = test_ds.batch(config.batch_size).prefetch(AUTO)"
      ],
      "metadata": {
        "id": "UoRyjihc57st"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "uy52Gjbf67eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_augmentation_model():\n",
        "  # Build the data augmentation model\n",
        "  data_augmentation = keras.Sequential(\n",
        "      [\n",
        "          layers.Resizing(config.input_shape[0] + 20, config.input_shape[0] + 20),\n",
        "          layers.RandomCrop(config.image_size, config.image_size),\n",
        "          layers.RandomFlip('horizontal'),\n",
        "          layers.Rescaling(1 / 255.0)\n",
        "      ]\n",
        "  )\n",
        "  return data_augmentation"
      ],
      "metadata": {
        "id": "kbvViJGQ640E"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The ShiftViT architecture**\n",
        "  - The architecture, as shown in Fig. 1, is inspired by the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.\n",
        "  - The authors propose a modular architecture with 4 stages.\n",
        "  - Each stage operates on its own spatial size, creating a hierarchical architecture.\n",
        "\n",
        "- **Input Image Processing**\n",
        "  - An input image of size HxWx3 is split into non-overlapping patches of size 4x4.\n",
        "  - This is done via the patchify layer, which results in individual tokens of feature size 48 (4x4x3).\n",
        "\n",
        "- **Stage Composition**\n",
        "  - Each stage comprises two parts:\n",
        "    - **Embedding Generation**\n",
        "    - **Stacked Shift Blocks**\n",
        "\n",
        "- **Detailed Discussion**\n",
        "  - The stages and modules are discussed in detail in the sections that follow.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "- **Architecture Inspiration**:\n",
        "  - The proposed architecture is inspired by the Swin Transformer, which uses a hierarchical approach with shifted windows to process images.\n",
        "  \n",
        "- **Modular and Hierarchical Design**:\n",
        "  - The design includes 4 distinct stages, each working on different spatial resolutions of the image. This hierarchical structure allows for capturing features at multiple scales.\n",
        "  \n",
        "- **Input Processing**:\n",
        "  - The input image, typically in RGB format (HxWx3), is divided into small patches of size 4x4 pixels. This process, termed \"patchification,\" converts the image into smaller, manageable tokens with a feature size of 48.\n",
        "  \n",
        "- **Components of Each Stage**:\n",
        "  - Each stage of the architecture is composed of two main parts:\n",
        "    - **Embedding Generation**: This part is responsible for creating a representation (embedding) of the image patches.\n",
        "    - **Stacked Shift Blocks**: These blocks perform the core operations, replacing the attention mechanism with shift operations, as proposed by the authors.\n",
        "\n",
        "![](https://i.imgur.com/CHU40HX.png)\n",
        "\n",
        "## **The ShiftViT Block**\n",
        "  - **ShiftViT Block Overview**\n",
        "    - Each stage in the ShiftViT architecture comprises a Shift Block, as shown in Fig. 2.\n",
        "    - Fig. 3 provides a detailed view of the Shift Block.\n",
        "\n",
        "  - **Components of the Shift Block**\n",
        "    - **Shift Operation**\n",
        "    - **Linear Normalization**\n",
        "    - **MLP Layer**\n",
        "\n",
        "Explanation:\n",
        "\n",
        "- **ShiftViT Block in Each Stage**:\n",
        "  - Each stage in the ShiftViT architecture is built around a core component called the Shift Block. This is illustrated in Fig. 2.\n",
        "\n",
        "- **Detailed View of the Shift Block**:\n",
        "  - Fig. 3 presents a detailed schematic of the Shift Block, breaking down its internal structure.\n",
        "\n",
        "- **Key Components**:\n",
        "  - **Shift Operation**: This operation replaces the traditional attention mechanism, introducing a parameter-free shift-based approach to process the data.\n",
        "  - **Linear Normalization**: This step normalizes the data linearly, ensuring that the values are scaled appropriately for the next layer.\n",
        "  - **MLP Layer**: The Multi-Layer Perceptron (MLP) layer provides the computational depth, applying learned transformations to the input data.\n",
        "\n",
        "![](https://i.imgur.com/0q13pLu.png)"
      ],
      "metadata": {
        "id": "8nPCq95Y7waB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The MLP block"
      ],
      "metadata": {
        "id": "pDTk0KmF9Tdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(layers.Layer):\n",
        "  '''\n",
        "  Get the MLP layer for each shift block\n",
        "\n",
        "  Args:\n",
        "      mlp_expand_ratio (int): The ratio with the first feature map is expanded\n",
        "      mlp_dropout_rate (float): The rate for dropout\n",
        "  '''\n",
        "  def __init__(self, mlp_expand_ratio, mlp_dropout_rate, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.mlp_expand_ratio = mlp_expand_ratio\n",
        "    self.mlp_dropout_rate = mlp_dropout_rate\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_channels = input_shape[-1]\n",
        "    initial_filters = int(self.mlp_expand_ratio * input_channels)\n",
        "\n",
        "    self.mlp = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(\n",
        "                units=initial_filters,\n",
        "                activation='gelu',\n",
        "            ),\n",
        "            layers.Dropout(rate=self.mlp_dropout_rate),\n",
        "            layers.Dense(units=input_channels),\n",
        "            layers.Dropout(rate=self.mlp_dropout_rate),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.mlp(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "hX7vlVXf7uAz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The DropPath layer"
      ],
      "metadata": {
        "id": "meQRgK9xDTTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DropPath(keras.layers.Layer):\n",
        "  # A layer that randomly drops residual paths during training\n",
        "\n",
        "  def __init__(self, drop_path_prob, **kwargs):\n",
        "    '''\n",
        "    Args:\n",
        "        drop_path_prob (float): The probability of dropping a residual path\n",
        "    '''\n",
        "    super().__init__(**kwargs)\n",
        "    self.drop_path_prob = drop_path_prob\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    if training:\n",
        "      keep_prob = 1 - self.drop_path_prob\n",
        "      shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)  # Subtract 1 from the length here\n",
        "      random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
        "      random_tensor = tf.floor(random_tensor)\n",
        "      return (x / keep_prob) * random_tensor\n",
        "    return x\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {'drop_path_prob': self.drop_path_prob}\n",
        "    return config"
      ],
      "metadata": {
        "id": "5oLLA0cEDNXU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shift operation Block\n",
        "- **Procedure:**\n",
        "\n",
        " - Split the channels with the num_div parameter.\n",
        "  - Select each of the first four spilts and shift and pad them in the respective directions.\n",
        "  - After shifting and padding, we concatenate the channel back.\n",
        "\n",
        "![](https://i.imgur.com/PReeULP.gif)"
      ],
      "metadata": {
        "id": "a07H9BEJElNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# out[:, g * 0:g * 1, :, :-1] = x[:, g * 0:g : 1, :, 1:] # shift left\n",
        "# out[:, g * 1:g * 2, :, 1:] = x[:, g * 1:g * 2, :, :-1] # shift right\n",
        "# out[:, g * 2:g * 3, :-1, :] = x[:, g * 2:g * 3, 1:, :] # shift up\n",
        "# out[:, g * 3:g * 4, 1:, :] = x[:, g * 3:g * 4, :-1, :] # shift down\n",
        "\n",
        "# out[:, g * 4:, :, :] = x[:, g * 4:, :, :] # no shift"
      ],
      "metadata": {
        "id": "jbyXhthPEiVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShiftViTBlock(layers.Layer):\n",
        "  '''\n",
        "  A unit ShiftViT Block:\n",
        "\n",
        "    Args:\n",
        "        shift_pixel (int): The number of pixels to shift. Default to 1.\n",
        "        mlp_expand_ratio (int): The ratio with which MLP features are\n",
        "                                expanded. Default to 2.\n",
        "        mlp_dropout_rate (float): The dropout rate used in MLP.\n",
        "        num_div (int): The number of divisions of the feature map's channel.\n",
        "                       Totally, 4/num_div of channels will be shifted. Defaults to 12.\n",
        "        epsilon (float): Epsilon constant.\n",
        "        drop_path_prob (float): The drop probability for drop path.\n",
        "    '''\n",
        "\n",
        "  def __init__( # Fixed: Removed extra indentation\n",
        "      self,\n",
        "      epsilon,\n",
        "      drop_path_prob,\n",
        "      mlp_dropout_rate,\n",
        "      num_div=12,\n",
        "      shift_pixel=1,\n",
        "      mlp_expand_ratio=2,\n",
        "      **kwargs,\n",
        "  ):\n",
        "      super().__init__(**kwargs)\n",
        "      self.shift_pixel = shift_pixel\n",
        "      self.mlp_expand_ratio = mlp_expand_ratio\n",
        "      self.mlp_dropout_rate = mlp_dropout_rate\n",
        "      self.num_div = num_div\n",
        "      self.epsilon = epsilon\n",
        "      self.drop_path_prob = drop_path_prob\n",
        "\n",
        "  def build(self, input_shape):\n",
        "      self.H = input_shape[1]\n",
        "      self.W = input_shape[2]\n",
        "      self.C = input_shape[3]\n",
        "      self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)\n",
        "      self.drop_path = (\n",
        "          DropPath(drop_path_prob=self.drop_path_prob)\n",
        "          if self.drop_path_prob > 0.0\n",
        "          else layers.Activation('linear')\n",
        "      )\n",
        "      self.mlp = MLP(\n",
        "          mlp_expand_ratio=self.mlp_expand_ratio,\n",
        "          mlp_dropout_rate=self.mlp_dropout_rate,\n",
        "      )\n",
        "\n",
        "  def get_shift_pad(self, x, mode):\n",
        "        # Shifts the channels according to the mode chosen\n",
        "      if mode == 'left':\n",
        "          offset_height = 0\n",
        "          offset_width = 0\n",
        "          target_height = 0\n",
        "          target_width = self.shift_pixel\n",
        "      elif mode == 'right':\n",
        "          offset_height = 0\n",
        "          offset_width = self.shift_pixel\n",
        "          target_height = 0\n",
        "          target_width = self.shift_pixel\n",
        "      elif mode == 'up':\n",
        "          offset_height = 0\n",
        "          offset_width = 0\n",
        "          target_height = self.shift_pixel\n",
        "          target_width = 0\n",
        "      else:\n",
        "          offset_height = self.shift_pixel\n",
        "          offset_width = 0\n",
        "          target_height = self.shift_pixel\n",
        "          target_width = 0\n",
        "\n",
        "      crop = tf.image.crop_to_bounding_box(\n",
        "            x,\n",
        "            offset_height=offset_height,\n",
        "            offset_width=offset_width,\n",
        "            target_height=self.H - target_height,\n",
        "            target_width=self.W - target_width,\n",
        "      )\n",
        "      shift_pad = tf.image.pad_to_bounding_box(\n",
        "            crop,\n",
        "            offset_height=offset_height,\n",
        "            offset_width=offset_width,\n",
        "            target_height=self.H,\n",
        "            target_width=self.W,\n",
        "      )\n",
        "      return shift_pad\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "      # Split the feature maps\n",
        "      x_splits = tf.split(x, num_or_size_splits=self.C // self.num_div, axis=-1)\n",
        "\n",
        "      # Shift the feature maps\n",
        "      x_splits[0] = self.get_shift_pad(x_splits[0], mode='left')\n",
        "      x_splits[1] = self.get_shift_pad(x_splits[1], mode='right')\n",
        "      x_splits[2] = self.get_shift_pad(x_splits[2], mode='up')\n",
        "      x_splits[3] = self.get_shift_pad(x_splits[3], mode='down')\n",
        "\n",
        "      # Concatenate the shifted and unshifted feature maps\n",
        "      x = tf.concat(x_splits, axis=-1)\n",
        "\n",
        "      # Add the residual connection\n",
        "      shortcut = x\n",
        "      x = shortcut + self.drop_path(self.mlp(self.layer_norm(x)), training=training)\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "grs29WXh3dvU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The ShiftViT blocks\n",
        "\n",
        "![](https://i.imgur.com/FKy5NnD.png)\n"
      ],
      "metadata": {
        "id": "451-Pf31O_o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The PatchMerging layer"
      ],
      "metadata": {
        "id": "nYJUBZfUPG15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(layers.Layer):\n",
        "  '''\n",
        "  The Patch Merging Layer:\n",
        "  Args:\n",
        "      epsilon (float): The epsilon constant\n",
        "  '''\n",
        "  def __init__(self, epsilon, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    filters = 2 * input_shape[-1]\n",
        "    self.reduction = layers.Conv2D(\n",
        "        filters=filters,\n",
        "        kernel_size=2,\n",
        "        strides=2,\n",
        "        padding='same',\n",
        "        use_bias=False,\n",
        "    )\n",
        "    self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)\n",
        "\n",
        "  def call(self, x):\n",
        "    # Apply patch merging algorithm on the feature maps\n",
        "    x = self.layer_norm(x)\n",
        "    x = self.reduction(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "dAMxDMPwPQG3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacked Shift Blocks"
      ],
      "metadata": {
        "id": "aQmG-pOJPMbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This layer will have a different depth of stacking for different stages on the model\n",
        "class StackedShiftBlocks(layers.Layer):\n",
        "  '''\n",
        "  This layer containing stacked ShiftViTBlocks\n",
        "  Args:\n",
        "      epsilon (float): The epsilon constant\n",
        "      mlp_dropout_rate (float): The dropout rate used in MLP\n",
        "      num_shift_blocks (int): The number of shift vit used blocks for this stage\n",
        "      stochastic_depth_rate (float): The maximum drop path rate chosen\n",
        "      is_merge (boolean): A flag that determines the use of the Patch Merge layer after the shift vit blocks\n",
        "      num_div (int): The division of channels of the feature map. Defaults to 12\n",
        "      shift_pixel (int): The number of pixels to shift Defaults to 1\n",
        "      mlp_expand_ratio (int): The ratio with which the initial dense layer of the MLP is expanded defaults to 2\n",
        "  '''\n",
        "  def __init__(\n",
        "      self,\n",
        "      epsilon,\n",
        "      mlp_dropout_rate,\n",
        "      num_shift_blocks,\n",
        "      stochastic_depth_rate,\n",
        "      is_merge,\n",
        "      num_div=12,\n",
        "      shift_pixel=1,\n",
        "      mlp_expand_ratio=2,\n",
        "      **kwargs,\n",
        "  ):\n",
        "      super().__init__(**kwargs)\n",
        "      self.epsilon = epsilon\n",
        "      self.mlp_dropout_rate = mlp_dropout_rate\n",
        "      self.num_shift_blocks = num_shift_blocks\n",
        "      self.stochastic_depth_rate = stochastic_depth_rate\n",
        "      self.is_merge = is_merge\n",
        "      self.num_div = num_div\n",
        "      self.shift_pixel = shift_pixel\n",
        "      self.mlp_expand_ratio = mlp_expand_ratio\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # Calculate stochastic depth probabilities\n",
        "    dpr = [\n",
        "        x\n",
        "        for x in np.linspace(\n",
        "            start=0, stop=self.stochastic_depth_rate, num=self.num_shift_blocks\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Build the shift blocks as a list of ShiftViT Blocks\n",
        "    self.shift_blocks = list()\n",
        "    for num in range(self.num_shift_blocks):\n",
        "      self.shift_blocks.append(\n",
        "          ShiftViTBlock(\n",
        "              num_div=self.num_div,\n",
        "              epsilon=self.epsilon,\n",
        "              drop_path_prob=dpr[num],\n",
        "              mlp_dropout_rate=self.mlp_dropout_rate,\n",
        "              shift_pixel=self.shift_pixel,\n",
        "              mlp_expand_ratio=self.mlp_expand_ratio,\n",
        "          )\n",
        "      )\n",
        "\n",
        "      if self.is_merge:\n",
        "        self.patch_merge = PatchMerging(epsilon=self.epsilon)\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    for shift_block in self.shift_blocks:\n",
        "      x = shift_block(x, training=training)\n",
        "    if self.is_merge:\n",
        "      x = self.patch_merge(x)\n",
        "    return x\n",
        "\n",
        "  # Since this is a custom layer, we need to overwrite get_config(), so that model can easily be saved and loaded after training\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update(\n",
        "        {\n",
        "            'epsilon': self.epsilon,\n",
        "            'mlp_dropout_rate': self.mlp_dropout_rate,\n",
        "            'num_shift_blocks': self.num_shift_blocks,\n",
        "            'stochastic_depth_rate': self.stochastic_depth_rate,\n",
        "            'is_merge': self.is_merge,\n",
        "            'num_div': self.num_div,\n",
        "            'shift_pixel': self.shift_pixel,\n",
        "            'mlp_expand_ratio': self.mlp_expand_ratio,\n",
        "        }\n",
        "    )\n",
        "    return config"
      ],
      "metadata": {
        "id": "5Z-Qsz3Qj2Y0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The ShiftViT model"
      ],
      "metadata": {
        "id": "bv3vtNE8qlF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShiftViTModel(keras.Model):\n",
        "    \"\"\"The ShiftViT Model.\n",
        "\n",
        "    Args:\n",
        "        data_augmentation (keras.Model): A data augmentation model.\n",
        "        projected_dim (int): The dimension to which the patches of the image are\n",
        "            projected.\n",
        "        patch_size (int): The patch size of the images.\n",
        "        num_shift_blocks_per_stages (list[int]): A list of all the number of shit\n",
        "            blocks per stage.\n",
        "        epsilon (float): The epsilon constant.\n",
        "        mlp_dropout_rate (float): The dropout rate used in the MLP block.\n",
        "        stochastic_depth_rate (float): The maximum drop rate probability.\n",
        "        num_div (int): The number of divisions of the channesl of the feature\n",
        "            map. Defaults to 12.\n",
        "        shift_pixel (int): The number of pixel to shift. Default to 1.\n",
        "        mlp_expand_ratio (int): The ratio with which the initial mlp dense layer\n",
        "            is expanded to. Defaults to 2.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_augmentation,\n",
        "        projected_dim,\n",
        "        patch_size,\n",
        "        num_shift_blocks_per_stages,\n",
        "        epsilon,\n",
        "        mlp_dropout_rate,\n",
        "        stochastic_depth_rate,\n",
        "        num_div=12,\n",
        "        shift_pixel=1,\n",
        "        mlp_expand_ratio=2,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.patch_projection = layers.Conv2D(\n",
        "            filters=projected_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"same\",\n",
        "        )\n",
        "        self.stages = list()\n",
        "        for index, num_shift_blocks in enumerate(num_shift_blocks_per_stages):\n",
        "            if index == len(num_shift_blocks_per_stages) - 1:\n",
        "                # This is the last stage, do not use the patch merge here.\n",
        "                is_merge = False\n",
        "            else:\n",
        "                is_merge = True\n",
        "            # Build the stages.\n",
        "            self.stages.append(\n",
        "                StackedShiftBlocks(\n",
        "                    epsilon=epsilon,\n",
        "                    mlp_dropout_rate=mlp_dropout_rate,\n",
        "                    num_shift_blocks=num_shift_blocks,\n",
        "                    stochastic_depth_rate=stochastic_depth_rate,\n",
        "                    is_merge=is_merge,\n",
        "                    num_div=num_div,\n",
        "                    shift_pixel=shift_pixel,\n",
        "                    mlp_expand_ratio=mlp_expand_ratio,\n",
        "                )\n",
        "            )\n",
        "        self.global_avg_pool = layers.GlobalAveragePooling2D()\n",
        "\n",
        "        self.classifier = layers.Dense(config.num_classes)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"data_augmentation\": self.data_augmentation,\n",
        "                \"patch_projection\": self.patch_projection,\n",
        "                \"stages\": self.stages,\n",
        "                \"global_avg_pool\": self.global_avg_pool,\n",
        "                \"classifier\": self.classifier,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "    def _calculate_loss(self, data, training=False):\n",
        "        (images, labels) = data\n",
        "\n",
        "        # Augment the images\n",
        "        augmented_images = self.data_augmentation(images, training=training)\n",
        "\n",
        "        # Create patches and project the pathces.\n",
        "        projected_patches = self.patch_projection(augmented_images)\n",
        "\n",
        "        # Pass through the stages\n",
        "        x = projected_patches\n",
        "        for stage in self.stages:\n",
        "            x = stage(x, training=training)\n",
        "\n",
        "        # Get the logits.\n",
        "        x = self.global_avg_pool(x)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        # Calculate the loss and return it.\n",
        "        total_loss = self.compiled_loss(labels, logits)\n",
        "        return total_loss, labels, logits\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            total_loss, labels, logits = self._calculate_loss(\n",
        "                data=inputs, training=True\n",
        "            )\n",
        "\n",
        "        # Apply gradients.\n",
        "        train_vars = [\n",
        "            self.data_augmentation.trainable_variables,\n",
        "            self.patch_projection.trainable_variables,\n",
        "            self.global_avg_pool.trainable_variables,\n",
        "            self.classifier.trainable_variables,\n",
        "        ]\n",
        "        train_vars = train_vars + [stage.trainable_variables for stage in self.stages]\n",
        "\n",
        "        # Optimize the gradients.\n",
        "        grads = tape.gradient(total_loss, train_vars)\n",
        "        trainable_variable_list = []\n",
        "        for (grad, var) in zip(grads, train_vars):\n",
        "            for g, v in zip(grad, var):\n",
        "                trainable_variable_list.append((g, v))\n",
        "        self.optimizer.apply_gradients(trainable_variable_list)\n",
        "\n",
        "        # Update the metrics\n",
        "        self.compiled_metrics.update_state(labels, logits)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        _, labels, logits = self._calculate_loss(data=data, training=False)\n",
        "\n",
        "        # Update the metrics\n",
        "        self.compiled_metrics.update_state(labels, logits)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def call(self, images):\n",
        "        augmented_images = self.data_augmentation(images)\n",
        "        x = self.patch_projection(augmented_images)\n",
        "        for stage in self.stages:\n",
        "            x = stage(x, training=False)\n",
        "        x = self.global_avg_pool(x)\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "kJYe3UraqcrK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate the model"
      ],
      "metadata": {
        "id": "8FjCYMmJ18bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ShiftViTModel(\n",
        "    data_augmentation=get_augmentation_model(),\n",
        "    projected_dim=config.projection_dim,\n",
        "    patch_size=config.patch_size,\n",
        "    num_shift_blocks_per_stages=config.num_shift_blocks_per_stages,\n",
        "    epsilon=config.epsilon,\n",
        "    mlp_dropout_rate=config.mlp_dropout_rate,\n",
        "    stochastic_depth_rate=config.stochastic_depth_rate,\n",
        "    num_div=config.num_div,\n",
        "    shift_pixel=config.shift_pixel,\n",
        "    mlp_expand_ratio=config.mlp_expand_ratio,\n",
        ")"
      ],
      "metadata": {
        "id": "XnyM0_mW19dJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning rate schedule"
      ],
      "metadata": {
        "id": "cy7v45oJ5xz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
        "  # A LearningRateSchedule that uses a warmup cosine decay schedule\n",
        "\n",
        "  def __init__(self, lr_start, lr_max, warmup_steps, total_steps):\n",
        "    '''\n",
        "    Args:\n",
        "        lr_start (float): The initial learning rate\n",
        "        lr_max (float): The maximum learning rate\n",
        "        warmup_steps (int): The number of warmup steps\n",
        "        total_steps (int): The total number of steps\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.lr_start = lr_start\n",
        "    self.lr_max = lr_max\n",
        "    self.warmup_steps = warmup_steps\n",
        "    self.total_steps = total_steps\n",
        "    self.pi = tf.constant(np.pi)\n",
        "\n",
        "  def __call__(self, step):\n",
        "    # Check wether the total number of steps is larger than the warmup steps\n",
        "    if self.total_steps < self.warmup_steps:\n",
        "      raise ValueError(\n",
        "          f'Total number of steps {self.total_steps} must be' +\n",
        "          f'larger or equal to warmup steps {self.warmup_steps}.'\n",
        "      )\n",
        "\n",
        "    # 'cos_annealed_lr'\n",
        "    cos_annealed_lr = tf.cos(\n",
        "        self.pi\n",
        "        * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
        "        / tf.cast(self.total_steps - self.warmup_steps, tf.float32)\n",
        "    )\n",
        "\n",
        "    learning_rate = 0.5 * self.lr_max * (1 + cos_annealed_lr)\n",
        "\n",
        "    # Check weather warmup step is more than 0\n",
        "    if self.warmup_steps > 0:\n",
        "      if self.lr_max < self.lr_start:\n",
        "        raise ValueError(\n",
        "            f'lr_srart {self.lr_start} must be smaller or' +\n",
        "            f'equal to lr_max {self.lr_max}.'\n",
        "        )\n",
        "\n",
        "      slope = (self.lr_max - self.lr_start) / self.warmup_steps\n",
        "\n",
        "      warmup_rate = slope * tf.cast(step, tf.float32) + self.lr_start\n",
        "\n",
        "      learning_rate = tf.where(\n",
        "          step < self.warmup_steps,\n",
        "          warmup_rate,\n",
        "          learning_rate,\n",
        "      )\n",
        "\n",
        "    return tf.where(\n",
        "        step > self.total_steps, 0.0, learning_rate, name='learning_rate'\n",
        "    )\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {\n",
        "        'lr_start': self.lr_start,\n",
        "        'lr_max': self.lr_max,\n",
        "        'total_steps': self.total_steps,\n",
        "        'warmup_steps': self.warmup_steps,\n",
        "    }\n",
        "    return config"
      ],
      "metadata": {
        "id": "4_xCGWnd31jm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile and train the model"
      ],
      "metadata": {
        "id": "fBgptmI_-XXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_ds, _ = next(iter(train_ds))\n",
        "model(sample_ds, training=False)\n",
        "\n",
        "total_steps = int((len(x_train) / config.batch_size) * config.epochs)\n",
        "\n",
        "warmup_epoch_percentage = 0.15\n",
        "warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
        "\n",
        "scheduled_lrs = WarmUpCosine(\n",
        "    lr_start=1e-5,\n",
        "    lr_max=1e-3,\n",
        "    warmup_steps=warmup_steps,\n",
        "    total_steps=total_steps,\n",
        ")\n",
        "\n",
        "optimizer = tfa.optimizers.AdamW(\n",
        "    learning_rate=scheduled_lrs, weight_decay=1e-2\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), # Fixed typo here\n",
        "    metrics=[\n",
        "        keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
        "        keras.metrics.SparseTopKCategoricalAccuracy(5, name='top-5-accuracy'),\n",
        "    ],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=config.epochs,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=5,\n",
        "            mode='auto',\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "\n",
        "print('TESTING')\n",
        "loss, acc_top1, acc_top5 = model.evaluate(test_ds)\n",
        "print(f'loss: {loss:0.02f}')\n",
        "print(f'Top 1 test accuracy: {acc_top1*100:0.2f}%')\n",
        "print(f'Top 5 test accuracy: {acc_top5*100:0.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIQBSWbX-ZdV",
        "outputId": "01e95b74-c5d8-4d8f-9c37-08800298f3ce"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "157/157 [==============================] - 930s 6s/step - loss: 2.0639 - accuracy: 0.2480 - top-5-accuracy: 0.7374 - val_loss: 1.8980 - val_accuracy: 0.3198 - val_top-5-accuracy: 0.8225\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 897s 6s/step - loss: 1.9060 - accuracy: 0.3098 - top-5-accuracy: 0.8201 - val_loss: 1.8782 - val_accuracy: 0.3134 - val_top-5-accuracy: 0.8181\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 893s 6s/step - loss: 1.9061 - accuracy: 0.3065 - top-5-accuracy: 0.8219 - val_loss: 1.9600 - val_accuracy: 0.2926 - val_top-5-accuracy: 0.8045\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 1129s 7s/step - loss: 1.9643 - accuracy: 0.2960 - top-5-accuracy: 0.8131 - val_loss: 2.0548 - val_accuracy: 0.2750 - val_top-5-accuracy: 0.8023\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 1016s 6s/step - loss: 2.2271 - accuracy: 0.1811 - top-5-accuracy: 0.6819 - val_loss: 2.3026 - val_accuracy: 0.0952 - val_top-5-accuracy: 0.5018\n",
            "TESTING\n",
            "40/40 [==============================] - 77s 2s/step - loss: 2.3026 - accuracy: 0.1000 - top-5-accuracy: 0.5000\n",
            "loss: 2.30\n",
            "Top 1 test accuracy: 10.00%\n",
            "Top 5 test accuracy: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- train on 50 epochs to get better results"
      ],
      "metadata": {
        "id": "wjeMPWRqQYBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save trained model"
      ],
      "metadata": {
        "id": "JlZdS5q68Vsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('ShiftViT')"
      ],
      "metadata": {
        "id": "0FAG2uuf8YJ4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model inference"
      ],
      "metadata": {
        "id": "TLPgrODQHAjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download sample data for inference\n",
        "!wget -q 'https://tinyurl.com/2p9483sw' -O inference_set.zip\n",
        "!unzip -q inference_set.zip"
      ],
      "metadata": {
        "id": "3rKXpGYXHB0g"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved model\n",
        "saved_model = tf.keras.models.load_model(\n",
        "    'ShiftViT',\n",
        "    custom_objects={'WarmUpCosine': WarmUpCosine, 'AdamW': tfa.optimizers.AdamW},\n",
        ")"
      ],
      "metadata": {
        "id": "yt6PDLcvHiF7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for inference\n",
        "def process_image(img_path):\n",
        "  img = tf.io.read_file(img_path)\n",
        "\n",
        "  img = tf.io.decode_jpeg(img, channels=3)\n",
        "\n",
        "  img = tf.image.resize(\n",
        "      img, [config.input_shape[0], config.input_shape[1]], method='nearest'\n",
        "  )\n",
        "  return img\n",
        "\n",
        "def create_tf_dataset(image_dir):\n",
        "  data_dir = pathlib.Path(image_dir)\n",
        "\n",
        "  predict_ds = tf.data.Dataset.list_files(str(data_dir / '*.jpg'), shuffle=False)\n",
        "\n",
        "  predict_ds = predict_ds.map(process_image, num_parallel_calls=AUTO)\n",
        "\n",
        "  predict_ds = predict_ds.batch(config.tf_ds_batch_size).prefetch(AUTO)\n",
        "  return predict_ds\n",
        "\n",
        "def predict(predict_ds):\n",
        "  logits = saved_model.predict(predict_ds)\n",
        "\n",
        "  probabilities = tf.nn.softmax(logits)\n",
        "  return probabilities\n",
        "\n",
        "def get_predicted_class(probabilities):\n",
        "  pred_label = np.argmax(probabilities)\n",
        "  predicted_class = config.label_map[pred_label]\n",
        "  return predicted_class\n",
        "\n",
        "def get_confidence_scores(probabilities):\n",
        "  labels = np.argsort(probabilities)[::-1]\n",
        "  confidences = {\n",
        "      config.label_map[label]: np.round((probabilities[label]) * 100, 2)\n",
        "      for label in labels\n",
        "  }\n",
        "\n",
        "  return confidences"
      ],
      "metadata": {
        "id": "7jr3rLpdIFo1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions\n",
        "img_dir = 'inference_set'\n",
        "predict_ds = create_tf_dataset(img_dir)\n",
        "probabilities = predict(predict_ds)\n",
        "print(f'probabilities: {probabilities[0]}')\n",
        "\n",
        "confidences = get_confidence_scores(probabilities[0])\n",
        "print(confidences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NKxx4OYOgiG",
        "outputId": "69984d68-2d6d-46dc-de40-0093153db346"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "probabilities: [0.10011393 0.10000818 0.10014922 0.10013884 0.09985273 0.09997135\n",
            " 0.09978589 0.09992218 0.10000063 0.10005701]\n",
            "{'bird': 10.01, 'cat': 10.01, 'airplane': 10.01, 'truck': 10.01, 'automobile': 10.0, 'ship': 10.0, 'dog': 10.0, 'horse': 9.99, 'deer': 9.99, 'frog': 9.98}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View predictions\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images in predict_ds:\n",
        "  for i in range(min(6, probabilities.shape[0])):\n",
        "    ax = plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(images[i].numpy().astype('uint8'))\n",
        "    predicted_class = get_predicted_class(probabilities[i])\n",
        "    plt.title(predicted_class)\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "6Cq25T53Ph4I",
        "outputId": "f0680fed-a50e-41bd-c7eb-7f2152fbc243"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rD8dn_rgQLnw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}