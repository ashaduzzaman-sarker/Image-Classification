{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOiWnuO6NRCqk1mz/PJZKgo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashaduzzaman-sarker/Image-Classification/blob/main/Image_classification_with_EANet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image classification with EANet (External Attention Transformer) on the CIFAR-100 dataset\n",
        "\n",
        "**Author:** [Ashaduzzaman Piash](\n",
        "https://github.com/ashaduzzaman-sarker/)\n",
        "<br>\n",
        "**Date created:** 19/06/2024\n",
        "<br>\n",
        "**Reference:**\n",
        "[Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks](\n",
        "https://doi.org/10.48550/arXiv.2105.02358)"
      ],
      "metadata": {
        "id": "9YViJBiGaK-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This example demonstrates the implementation of the EANet model for image classification using the CIFAR-100 dataset. The EANet model introduces a novel attention mechanism known as external attention. Unlike traditional self-attention mechanisms, external attention uses two external, small, learnable, and shared memories. These memories are implemented using two cascaded linear layers and two normalization layers, making the mechanism both simple and efficient to implement.\n",
        "\n",
        "The external attention mechanism provides several advantages:\n",
        "\n",
        "- **Simplicity**: It can be implemented with minimal modifications to existing architectures.\n",
        "- **Efficiency**: It has linear complexity, as it implicitly considers the correlations between all samples without the quadratic complexity of self-attention.\n",
        "\n",
        "By leveraging these benefits, EANet can effectively replace self-attention in existing models, offering a more computationally efficient alternative while maintaining or improving performance. This implementation will show how to integrate EANet with a standard image classification pipeline on the CIFAR-100 dataset.\n",
        "\n",
        "### Key Components of EANet:\n",
        "1. **External Attention**: Utilizes two small, shared memories to capture attention, leading to linear complexity.\n",
        "2. **Cascaded Linear Layers**: Two linear layers that facilitate the attention mechanism.\n",
        "3. **Normalization Layers**: Two normalization layers that ensure stability and improved convergence.\n",
        "\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSRRx7ntV0wgQwDZYFdC39WlZx9wOQCt0EDqA&s)\n",
        "\n",
        "![](https://user-images.githubusercontent.com/17668390/141291708-7c3cd892-d508-4cca-8306-a8b06a38c158.png)"
      ],
      "metadata": {
        "id": "Z6hRPWQSbtq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "4FfNhyNee_YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade keras"
      ],
      "metadata": {
        "id": "jnVp2lnEfh40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops"
      ],
      "metadata": {
        "id": "ohfLktr1fBTF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data\n",
        "\n",
        "**CIFAR-100 Dataset:**\n",
        "\n",
        "The CIFAR-100 dataset is a well-known benchmark in the field of image classification, containing 100 classes with 600 images each. This dataset provides a robust platform to demonstrate the effectiveness of EANet."
      ],
      "metadata": {
        "id": "Z9CkzojGfRgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(f'x_train shape: {x_train.shape} | y_train shape: {y_train.shape}')\n",
        "print(f'x_test shape: {x_test.shape} | y_test shape: {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh8KLHpJfP7b",
        "outputId": "237801db-56b1-4850-8969-a54897f3a029"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3) | y_train shape: (50000, 100)\n",
            "x_test shape: (10000, 32, 32, 3) | y_test shape: (10000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure the hyperparameters"
      ],
      "metadata": {
        "id": "DHPUNklzg_Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_decay = 0.0001\n",
        "learning_rate = 0.001\n",
        "label_smoothing = 0.1\n",
        "validation_split = 0.2\n",
        "batch_size = 128\n",
        "num_epochs = 50\n",
        "patch_size = 2  # Size of the patches to be extracted from input images\n",
        "num_patches = (input_shape[0] // patch_size) ** 2  # Number of patch\n",
        "embedding_dim = 64\n",
        "mlp_dim = 64\n",
        "dim_coefficient = 4\n",
        "num_heads = 4\n",
        "attention_dropout = 0.2\n",
        "projection_dropout = 0.2\n",
        "num_transformer_blocks = 8  # Number of repetitions of the transformer layer\n",
        "\n",
        "print(f'Patch size: {patch_size} X {patch_size} = {patch_size ** 2}')\n",
        "print(f'Patches per image: {num_patches}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JwxBFFnhBi2",
        "outputId": "3f9cf637-64f2-4db2-923c-75222ca2950d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patch_size: 2 X 2 = 4\n",
            "Patches per image: 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use data augmentation"
      ],
      "metadata": {
        "id": "x5mAc7WDjK-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.RandomFlip('horizontal'),\n",
        "        layers.RandomRotation(factor=0.1),\n",
        "        layers.RandomContrast(factor=0.1),\n",
        "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "    ],\n",
        "    name='data_augmentation',\n",
        ")\n",
        "\n",
        "# Compute the mean and the variance of the training data for normalization\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "metadata": {
        "id": "x8LK9-kdg1KC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the patch extraction and encoding layer"
      ],
      "metadata": {
        "id": "4Ho6FXXfkJzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchExtract(layers.Layer):\n",
        "  def __init__(self, patch_size, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "  def call(self, x):\n",
        "    B, C = ops.shape(x)[0], ops.shape(x)[-1]\n",
        "    x = ops.image.extract_patches(x, self.patch_size)\n",
        "    x = ops.reshape(x, (B, -1, self.patch_size * self.patch_size * C))\n",
        "    return x\n",
        "\n",
        "class PatchEmbedding(layers.Layer):\n",
        "  def __init__(self, num_patch, embed_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.num_patch = num_patch\n",
        "    self.proj = layers.Dense(embed_dim)\n",
        "    self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
        "\n",
        "  def call(self, patch):\n",
        "    pos = ops.arange(start=0, stop=self.num_patch, step=1)\n",
        "    return self.proj(patch) + self.pos_embed(pos)\n"
      ],
      "metadata": {
        "id": "3N6vcafpgtpT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of the external attention block"
      ],
      "metadata": {
        "id": "V9F2wNKDmoI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def external_attention(\n",
        "    x,\n",
        "    dim,\n",
        "    num_heads,\n",
        "    dim_coefficient=4,\n",
        "    attention_dropout=0,\n",
        "    projection_dropout=0,\n",
        "):\n",
        "    _, num_patch, channel = x.shape\n",
        "    assert dim % num_heads == 0\n",
        "    num_heads = num_heads * dim_coefficient\n",
        "\n",
        "    x = layers.Dense(dim * dim_coefficient)(x)\n",
        "    # Create tensor [batch_size, num_patchs, num_heads, dim*dim_coefficient//num_heads]\n",
        "    x = ops.reshape(x, (-1, num_patch, num_heads, dim * dim_coefficient // num_heads))\n",
        "    x = ops.transpose(x, axes=[0, 2, 1, 3])\n",
        "    # A linear layer M_k\n",
        "    attn = layers.Dense(dim // dim_coefficient)(x)\n",
        "    # Normalize attention map\n",
        "    attn = layers.Softmax(axis=2)(attn)\n",
        "    # Double-normalization\n",
        "    attn = layers.Lambda(\n",
        "        lambda attn: ops.divide(\n",
        "            attn,\n",
        "            ops.convert_to_tensor(1e-9) + ops.sum(attn, axis=-1, keepdims=True),\n",
        "        )\n",
        "    )(attn)\n",
        "    attn = layers.Dropout(attention_dropout)(attn)\n",
        "    # A linear layer M_v\n",
        "    x = layers.Dense(dim * dim_coefficient // num_heads)(attn)\n",
        "    x = ops.transpose(x, axes=[0, 2, 1, 3])\n",
        "    x = ops.reshape(x, [-1, num_patch, dim * dim_coefficient])\n",
        "    # A linear layer to project original dim\n",
        "    x = layers.Dense(dim)(x)\n",
        "    x = layers.Dropout(projection_dropout)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "WLMX-9JLbV_Q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the MLP block"
      ],
      "metadata": {
        "id": "tcrzqZtU6GfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, embedding_dim, mlp_dim, drop_rate=0.2):\n",
        "  x = layers.Dense(mlp_dim, activation=ops.gelu)(x)\n",
        "  x = layers.Dropout(drop_rate)(x)\n",
        "  x = layers.Dense(embedding_dim)(x)\n",
        "  x = layers.Dropout(drop_rate)(x)\n",
        "  return x"
      ],
      "metadata": {
        "id": "qlkKDuAX6EUw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the Transformer block"
      ],
      "metadata": {
        "id": "wZ1Zp3ws6k43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(\n",
        "    x,\n",
        "    embedding_dim,\n",
        "    mlp_dim,\n",
        "    num_heads,\n",
        "    dim_coefficient,\n",
        "    attention_dropout,\n",
        "    projection_dropout,\n",
        "    attention_type='external_attention',\n",
        "):\n",
        "    residual_1 = x\n",
        "    x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
        "    if attention_type == 'external_attention':\n",
        "      x = external_attention(\n",
        "          x,\n",
        "          embedding_dim,\n",
        "          num_heads,\n",
        "          dim_coefficient,\n",
        "          attention_dropout,\n",
        "          projection_dropout,\n",
        "      )\n",
        "    elif attention_type == 'self_attention':\n",
        "      x = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads,\n",
        "          key_dim=embedding_dim,\n",
        "          dropout=attention_dropout,\n",
        "      )(x, x)\n",
        "    x = layers.add([x, residual_1])\n",
        "    residual_2 = x\n",
        "    x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
        "    x = mlp(x, embedding_dim, mlp_dim)\n",
        "    x = layers.add([x, residual_2])\n",
        "    return x"
      ],
      "metadata": {
        "id": "oHUokScg6lyG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of the EANet model\n",
        "\n",
        "- **EANet Model**: Utilizes external attention instead of traditional self-attention.\n",
        "- **Traditional Self-Attention Complexity**: O(d * N ** 2), where d is the embedding size and N is the number of patches.\n",
        "- **Redundancy in Self-Attention**: Most pixels are closely related to a few others, making an N-to-N attention matrix redundant.\n",
        "- **External Attention Module**: Proposed to address this redundancy.\n",
        "- **External Attention Complexity**: O(d * S * N), where d and S are hyper-parameters.\n",
        "- **Linear Complexity**: The algorithm is linear in the number of pixels, as d and S are constants.\n",
        "- **Equivalent to Drop Patch Operation**: Redundant and unimportant information in image patches is effectively ignored.\n",
        "\n",
        "![](https://user-images.githubusercontent.com/17668390/141291708-7c3cd892-d508-4cca-8306-a8b06a38c158.png)"
      ],
      "metadata": {
        "id": "OoZMgwAe8NB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(attention_type='external_attention'):\n",
        "  inputs = layers.Input(shape=input_shape)\n",
        "  # Image augment\n",
        "  x = data_augmentation(inputs)\n",
        "  # Extract patches\n",
        "  x = PatchExtract(patch_size)(x)\n",
        "  # Create patch embedding\n",
        "  x = PatchEmbedding(num_patches, embedding_dim)(x)\n",
        "  # Create Transformer block\n",
        "  for _ in range(num_transformer_blocks):\n",
        "    x = transformer_encoder(\n",
        "        x,\n",
        "        embedding_dim,\n",
        "        mlp_dim,\n",
        "        num_heads,\n",
        "        dim_coefficient,\n",
        "        attention_dropout,\n",
        "        projection_dropout,\n",
        "        attention_type,\n",
        "    )\n",
        "\n",
        "  x = layers.GlobalAveragePooling1D()(x)\n",
        "  outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "QCLM-Hai8KQl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on CIFAR-100 Dataset"
      ],
      "metadata": {
        "id": "idhZPA-m-jvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(attention_type='external_attention')\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
        "    optimizer=keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    metrics=[\n",
        "        keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "        keras.metrics.TopKCategoricalAccuracy(5, name='top-5-accuracy'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs,\n",
        "    validation_split=validation_split,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbV6rcmQ-lls",
        "outputId": "ed1025c7-2b1a-428f-aa9d-47043b839811"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 254ms/step - accuracy: 0.0376 - loss: 4.4732 - top-5-accuracy: 0.1417 - val_accuracy: 0.0569 - val_loss: 4.8607 - val_top-5-accuracy: 0.1957\n",
            "Epoch 2/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 265ms/step - accuracy: 0.0932 - loss: 4.0584 - top-5-accuracy: 0.2894 - val_accuracy: 0.0766 - val_loss: 4.9957 - val_top-5-accuracy: 0.2509\n",
            "Epoch 3/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.1284 - loss: 3.8808 - top-5-accuracy: 0.3546 - val_accuracy: 0.0841 - val_loss: 5.5828 - val_top-5-accuracy: 0.2561\n",
            "Epoch 4/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.1616 - loss: 3.7415 - top-5-accuracy: 0.4125 - val_accuracy: 0.0870 - val_loss: 5.6483 - val_top-5-accuracy: 0.2623\n",
            "Epoch 5/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 264ms/step - accuracy: 0.1829 - loss: 3.6414 - top-5-accuracy: 0.4497 - val_accuracy: 0.1131 - val_loss: 5.1672 - val_top-5-accuracy: 0.2860\n",
            "Epoch 6/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 248ms/step - accuracy: 0.1990 - loss: 3.5681 - top-5-accuracy: 0.4703 - val_accuracy: 0.1204 - val_loss: 5.0824 - val_top-5-accuracy: 0.3092\n",
            "Epoch 7/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 263ms/step - accuracy: 0.2151 - loss: 3.4984 - top-5-accuracy: 0.4927 - val_accuracy: 0.1217 - val_loss: 5.1613 - val_top-5-accuracy: 0.3135\n",
            "Epoch 8/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 265ms/step - accuracy: 0.2280 - loss: 3.4495 - top-5-accuracy: 0.5085 - val_accuracy: 0.1220 - val_loss: 5.3292 - val_top-5-accuracy: 0.3112\n",
            "Epoch 9/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 264ms/step - accuracy: 0.2357 - loss: 3.4094 - top-5-accuracy: 0.5165 - val_accuracy: 0.1243 - val_loss: 5.2368 - val_top-5-accuracy: 0.3234\n",
            "Epoch 10/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 264ms/step - accuracy: 0.2501 - loss: 3.3598 - top-5-accuracy: 0.5345 - val_accuracy: 0.1238 - val_loss: 5.2770 - val_top-5-accuracy: 0.3194\n",
            "Epoch 11/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 265ms/step - accuracy: 0.2580 - loss: 3.3148 - top-5-accuracy: 0.5525 - val_accuracy: 0.1377 - val_loss: 5.0257 - val_top-5-accuracy: 0.3499\n",
            "Epoch 12/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 248ms/step - accuracy: 0.2625 - loss: 3.2928 - top-5-accuracy: 0.5604 - val_accuracy: 0.1389 - val_loss: 5.1835 - val_top-5-accuracy: 0.3459\n",
            "Epoch 13/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 248ms/step - accuracy: 0.2692 - loss: 3.2569 - top-5-accuracy: 0.5720 - val_accuracy: 0.1353 - val_loss: 5.2038 - val_top-5-accuracy: 0.3391\n",
            "Epoch 14/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 264ms/step - accuracy: 0.2804 - loss: 3.2256 - top-5-accuracy: 0.5757 - val_accuracy: 0.1391 - val_loss: 5.1530 - val_top-5-accuracy: 0.3450\n",
            "Epoch 15/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 264ms/step - accuracy: 0.2872 - loss: 3.1970 - top-5-accuracy: 0.5867 - val_accuracy: 0.1462 - val_loss: 5.0762 - val_top-5-accuracy: 0.3442\n",
            "Epoch 16/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.2990 - loss: 3.1539 - top-5-accuracy: 0.6022 - val_accuracy: 0.1391 - val_loss: 5.2555 - val_top-5-accuracy: 0.3458\n",
            "Epoch 17/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 264ms/step - accuracy: 0.3007 - loss: 3.1388 - top-5-accuracy: 0.6039 - val_accuracy: 0.1479 - val_loss: 5.0933 - val_top-5-accuracy: 0.3498\n",
            "Epoch 18/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 248ms/step - accuracy: 0.3068 - loss: 3.1202 - top-5-accuracy: 0.6084 - val_accuracy: 0.1507 - val_loss: 5.0870 - val_top-5-accuracy: 0.3612\n",
            "Epoch 19/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 248ms/step - accuracy: 0.3135 - loss: 3.0886 - top-5-accuracy: 0.6179 - val_accuracy: 0.1571 - val_loss: 5.1149 - val_top-5-accuracy: 0.3650\n",
            "Epoch 20/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 263ms/step - accuracy: 0.3141 - loss: 3.0865 - top-5-accuracy: 0.6212 - val_accuracy: 0.1578 - val_loss: 5.2511 - val_top-5-accuracy: 0.3682\n",
            "Epoch 21/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 264ms/step - accuracy: 0.3233 - loss: 3.0550 - top-5-accuracy: 0.6264 - val_accuracy: 0.1652 - val_loss: 5.0823 - val_top-5-accuracy: 0.3833\n",
            "Epoch 22/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.3315 - loss: 3.0223 - top-5-accuracy: 0.6407 - val_accuracy: 0.1621 - val_loss: 5.0883 - val_top-5-accuracy: 0.3813\n",
            "Epoch 23/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 264ms/step - accuracy: 0.3359 - loss: 3.0086 - top-5-accuracy: 0.6434 - val_accuracy: 0.1598 - val_loss: 5.2277 - val_top-5-accuracy: 0.3704\n",
            "Epoch 24/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.3414 - loss: 2.9788 - top-5-accuracy: 0.6525 - val_accuracy: 0.1507 - val_loss: 5.3989 - val_top-5-accuracy: 0.3584\n",
            "Epoch 25/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 264ms/step - accuracy: 0.3458 - loss: 2.9791 - top-5-accuracy: 0.6525 - val_accuracy: 0.1643 - val_loss: 5.0680 - val_top-5-accuracy: 0.3885\n",
            "Epoch 26/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.3457 - loss: 2.9755 - top-5-accuracy: 0.6545 - val_accuracy: 0.1645 - val_loss: 5.0785 - val_top-5-accuracy: 0.3865\n",
            "Epoch 27/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.3523 - loss: 2.9436 - top-5-accuracy: 0.6592 - val_accuracy: 0.1655 - val_loss: 5.0798 - val_top-5-accuracy: 0.3815\n",
            "Epoch 28/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 264ms/step - accuracy: 0.3594 - loss: 2.9232 - top-5-accuracy: 0.6698 - val_accuracy: 0.1752 - val_loss: 5.0374 - val_top-5-accuracy: 0.3949\n",
            "Epoch 29/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.3582 - loss: 2.9169 - top-5-accuracy: 0.6694 - val_accuracy: 0.1696 - val_loss: 5.0348 - val_top-5-accuracy: 0.3911\n",
            "Epoch 30/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.3633 - loss: 2.8959 - top-5-accuracy: 0.6739 - val_accuracy: 0.1643 - val_loss: 5.1216 - val_top-5-accuracy: 0.3883\n",
            "Epoch 31/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 264ms/step - accuracy: 0.3707 - loss: 2.8797 - top-5-accuracy: 0.6772 - val_accuracy: 0.1576 - val_loss: 5.4539 - val_top-5-accuracy: 0.3692\n",
            "Epoch 32/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 248ms/step - accuracy: 0.3747 - loss: 2.8632 - top-5-accuracy: 0.6808 - val_accuracy: 0.1628 - val_loss: 5.2673 - val_top-5-accuracy: 0.3799\n",
            "Epoch 33/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 248ms/step - accuracy: 0.3805 - loss: 2.8528 - top-5-accuracy: 0.6839 - val_accuracy: 0.1669 - val_loss: 5.1692 - val_top-5-accuracy: 0.3888\n",
            "Epoch 34/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 248ms/step - accuracy: 0.3826 - loss: 2.8418 - top-5-accuracy: 0.6868 - val_accuracy: 0.1740 - val_loss: 5.0529 - val_top-5-accuracy: 0.4042\n",
            "Epoch 35/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 264ms/step - accuracy: 0.3845 - loss: 2.8288 - top-5-accuracy: 0.6954 - val_accuracy: 0.1752 - val_loss: 4.9813 - val_top-5-accuracy: 0.4036\n",
            "Epoch 36/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 249ms/step - accuracy: 0.3935 - loss: 2.8068 - top-5-accuracy: 0.6996 - val_accuracy: 0.1686 - val_loss: 5.1296 - val_top-5-accuracy: 0.3980\n",
            "Epoch 37/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 248ms/step - accuracy: 0.3913 - loss: 2.8155 - top-5-accuracy: 0.6948 - val_accuracy: 0.1805 - val_loss: 4.9819 - val_top-5-accuracy: 0.4133\n",
            "Epoch 38/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 249ms/step - accuracy: 0.3925 - loss: 2.8012 - top-5-accuracy: 0.6987 - val_accuracy: 0.1696 - val_loss: 5.1469 - val_top-5-accuracy: 0.3988\n",
            "Epoch 39/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 264ms/step - accuracy: 0.3961 - loss: 2.7806 - top-5-accuracy: 0.7056 - val_accuracy: 0.1748 - val_loss: 5.0065 - val_top-5-accuracy: 0.4074\n",
            "Epoch 40/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 264ms/step - accuracy: 0.3988 - loss: 2.7802 - top-5-accuracy: 0.7035 - val_accuracy: 0.1804 - val_loss: 5.0388 - val_top-5-accuracy: 0.4070\n",
            "Epoch 41/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 264ms/step - accuracy: 0.4060 - loss: 2.7580 - top-5-accuracy: 0.7102 - val_accuracy: 0.1704 - val_loss: 5.2237 - val_top-5-accuracy: 0.4051\n",
            "Epoch 42/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.4100 - loss: 2.7415 - top-5-accuracy: 0.7150 - val_accuracy: 0.1761 - val_loss: 5.2119 - val_top-5-accuracy: 0.4074\n",
            "Epoch 43/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 249ms/step - accuracy: 0.4094 - loss: 2.7394 - top-5-accuracy: 0.7168 - val_accuracy: 0.1860 - val_loss: 5.0071 - val_top-5-accuracy: 0.4256\n",
            "Epoch 44/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 264ms/step - accuracy: 0.4140 - loss: 2.7333 - top-5-accuracy: 0.7164 - val_accuracy: 0.1763 - val_loss: 5.2826 - val_top-5-accuracy: 0.3986\n",
            "Epoch 45/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 249ms/step - accuracy: 0.4145 - loss: 2.7171 - top-5-accuracy: 0.7228 - val_accuracy: 0.1763 - val_loss: 5.2817 - val_top-5-accuracy: 0.3976\n",
            "Epoch 46/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 265ms/step - accuracy: 0.4220 - loss: 2.7092 - top-5-accuracy: 0.7227 - val_accuracy: 0.1793 - val_loss: 5.2519 - val_top-5-accuracy: 0.4079\n",
            "Epoch 47/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.4208 - loss: 2.6936 - top-5-accuracy: 0.7271 - val_accuracy: 0.1850 - val_loss: 5.0899 - val_top-5-accuracy: 0.4237\n",
            "Epoch 48/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.4293 - loss: 2.6808 - top-5-accuracy: 0.7323 - val_accuracy: 0.1886 - val_loss: 5.0738 - val_top-5-accuracy: 0.4251\n",
            "Epoch 49/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 265ms/step - accuracy: 0.4260 - loss: 2.6676 - top-5-accuracy: 0.7389 - val_accuracy: 0.1812 - val_loss: 5.1386 - val_top-5-accuracy: 0.4190\n",
            "Epoch 50/50\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 265ms/step - accuracy: 0.4315 - loss: 2.6581 - top-5-accuracy: 0.7360 - val_accuracy: 0.1875 - val_loss: 5.0288 - val_top-5-accuracy: 0.4220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the final results of the test on CIFAR-100."
      ],
      "metadata": {
        "id": "CdwSjUnjAzI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test loss: {round(loss, 2)}')\n",
        "print(f'Test accuracy: {round(accuracy * 100, 2)}%')\n",
        "print(f'Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIJHQLf__c8F",
        "outputId": "bbb9629f-15c1-4708-c9b7-db5798cefb2c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.1925 - loss: 4.9667 - top-5-accuracy: 0.4333\n",
            "Test loss: 5.02\n",
            "Test accuracy: 18.63%\n",
            "Test top 5 accuracy: 42.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "- **Replacement in ViT**: EANet replaces self-attention in Vision Transformer (ViT) with external attention.\n",
        "\n",
        "- **ViT Performance**:\n",
        "  - Test Top-5 Accuracy: ~73%\n",
        "  - Test Top-1 Accuracy: ~41%\n",
        "  - Parameters: 0.6M\n",
        "  - Training Epochs: 50\n",
        "\n",
        "- **EANet Performance**:\n",
        "  - Test Top-5 Accuracy: ~73%\n",
        "  - Test Top-1 Accuracy: ~43%\n",
        "  - Parameters: 0.3M\n",
        "  - Training Epochs: 50\n",
        "\n",
        "- **Effectiveness Demonstrated**: EANet achieves similar or better accuracy with half the parameters, proving the efficiency and effectiveness of external attention."
      ],
      "metadata": {
        "id": "ssWFCeMwBtQW"
      }
    }
  ]
}