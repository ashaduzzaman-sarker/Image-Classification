{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashaduzzaman-sarker/Image-Classification/blob/main/Training_a_ViT_from_scratch_on_smaller_datasets_with_shifted_patch_tokenization_and_locality_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_wibZ0Z6tS5"
      },
      "source": [
        "# Training a ViT from scratch on smaller datasets with shifted patch tokenization and locality self-attention\n",
        "\n",
        "**Author:** [Ashaduzzaman Sarker](https://github.com/ashaduzzaman-sarker/)\n",
        "<br>\n",
        "**Date created:** 29/06/2024\n",
        "<br>\n",
        "**Reference:**\n",
        "- [Vision Transformer for Small-Size Datasets](\n",
        "https://doi.org/10.48550/arXiv.2112.13492)\n",
        "\n",
        "- [Keras](https://keras.io/examples/vision/vit_small_ds/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOrMVQQN7wAw"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In the paper [\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"](\n",
        "https://doi.org/10.48550/arXiv.2010.11929\n",
        "), the authors highlight the data-intensive nature of Vision Transformers (ViT). To outperform state-of-the-art Convolutional Neural Network (CNN) models, ViTs must be pretrained on large datasets like JFT300M and subsequently fine-tuned on medium-sized datasets such as ImageNet.\n",
        "\n",
        "ViTs require more data primarily because their self-attention layer lacks locality inductive bias. This bias refers to the concept that image pixels are locally correlated and their correlation maps are translation-invariant. In contrast, CNNs use spatial sliding windows to analyze images, enabling them to achieve better results with smaller datasets.\n",
        "[Vision Transformer for Small-Size Datasets](\n",
        "https://doi.org/10.48550/arXiv.2112.13492) aims to address the lack of locality inductive bias in ViTs. The authors propose two main ideas to enhance ViT performance on smaller datasets:\n",
        "\n",
        "- **Shifted Patch Tokenization**\n",
        "- **Locality Self Attention**\n",
        "\n",
        "This example implements the concepts from the paper, with significant inspiration drawn from the \"Image classification with Vision Transformer\" model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WeQ8YQo8c6m"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhm8VomT8u43"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nmENc0q8kq9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "keras.utils.set_random_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7kCzJGv-qqo"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmeBU90P9ouU"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 100\n",
        "INPUT_SHAPE = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f'x_train shape: {x_train.shape} - y_train shape: {y_train.shape}')\n",
        "print(f'x_test shape: {x_test.shape} - y_test shape: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLCljN6oBKca"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qSuBWH_BNOx"
      },
      "outputs": [],
      "source": [
        "# DATA\n",
        "BUFFER_SIZE = 512\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# AUGMENTATION\n",
        "IMAGE_SIZE = 72\n",
        "PATCH_SIZE = 6\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 1 #50\n",
        "\n",
        "# ARCHITECTIRE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "TRANSFORMER_LAYERS = 8\n",
        "PROJECTION_DIM = 64\n",
        "NUM_HEADS = 4\n",
        "TRANSFORMER_UNITS = [\n",
        "    PROJECTION_DIM * 2,\n",
        "    PROJECTION_DIM,\n",
        "]\n",
        "MLP_HEAD_UNITS = [2048, 1024]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz1EsQbTCZF2"
      },
      "source": [
        "## Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6szrJ2rCWft"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        layers.RandomFlip('horizontal'),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "    ],\n",
        "    name='data_augmentation',\n",
        ")\n",
        "\n",
        "# Compute the mean and the variace of the training data for normalization\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJFfn4ueEpyy"
      },
      "source": [
        "## Implement Shifted Patch Tokenization (STP) for Vision Transformers\n",
        "\n",
        "Shifted Patch Tokenization (STP) is designed to address the low receptive field of Vision Transformers (ViTs) by incorporating more local context into the tokens. Here's a detailed explanation of the steps involved in STP:\n",
        "\n",
        "1. **Start with an Image:**\n",
        "   - Begin with the original input image.\n",
        "\n",
        "2. **Shift the Image in Diagonal Directions:**\n",
        "   - Create several versions of the image by shifting it in different diagonal directions (e.g., top-left, top-right, bottom-left, bottom-right).\n",
        "\n",
        "3. **Concat the Diagonally Shifted Images with the Original Image:**\n",
        "   - Concatenate the diagonally shifted images with the original image to form a larger composite image.\n",
        "\n",
        "4. **Extract Patches of the Concatenated Images:**\n",
        "   - From the composite image, extract patches. This step ensures that each patch now contains more local context due to the inclusion of shifted versions of the original image.\n",
        "\n",
        "5. **Flatten the Spatial Dimension of All Patches:**\n",
        "   - Flatten the spatial dimensions of each patch to convert them into a 1D sequence. This step prepares the patches for input into the transformer model.\n",
        "\n",
        "6. **Layer Normalize the Flattened Patches and then Project Them:**\n",
        "   - Apply layer normalization to the flattened patches to stabilize training and improve convergence.\n",
        "   - Linearly project the normalized patches to the desired token dimension. This step converts the patches into tokens that can be processed by the transformer model.\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/bUnHxd0.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGCVcFpOEmpf"
      },
      "outputs": [],
      "source": [
        "class ShiftedPatchTokenization(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_patches=NUM_PATCHES,\n",
        "        projection_dim=PROJECTION_DIM,\n",
        "        vanilla=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.half_patch = patch_size // 2\n",
        "        self.flatten_patches = layers.Reshape((num_patches, -1))\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
        "\n",
        "    def crop_shift_pad(self, images, mode):\n",
        "        # Build the diagonally shifted images\n",
        "        if mode == \"left-up\":\n",
        "            crop_height = self.half_patch\n",
        "            crop_width = self.half_patch\n",
        "            shift_height = 0\n",
        "            shift_width = 0\n",
        "        elif mode == \"left-down\":\n",
        "            crop_height = 0\n",
        "            crop_width = self.half_patch\n",
        "            shift_height = self.half_patch\n",
        "            shift_width = 0\n",
        "        elif mode == \"right-up\":\n",
        "            crop_height = self.half_patch\n",
        "            crop_width = 0\n",
        "            shift_height = 0\n",
        "            shift_width = self.half_patch\n",
        "        else:\n",
        "            crop_height = 0\n",
        "            crop_width = 0\n",
        "            shift_height = self.half_patch\n",
        "            shift_width = self.half_patch\n",
        "\n",
        "        # Crop the shifted images and pad them\n",
        "        crop = tf.image.crop_to_bounding_box(\n",
        "            images,\n",
        "            offset_height=crop_height,\n",
        "            offset_width=crop_width,\n",
        "            target_height=self.image_size - self.half_patch,\n",
        "            target_width=self.image_size - self.half_patch,\n",
        "        )\n",
        "        shift_pad = tf.image.pad_to_bounding_box(\n",
        "            crop,\n",
        "            offset_height=shift_height,\n",
        "            offset_width=shift_width,\n",
        "            target_height=self.image_size,\n",
        "            target_width=self.image_size,\n",
        "        )\n",
        "        return shift_pad\n",
        "\n",
        "    def call(self, images):\n",
        "        if not self.vanilla:\n",
        "            # Concat the shifted images with the original image\n",
        "            images = tf.concat(\n",
        "                [\n",
        "                    images,\n",
        "                    self.crop_shift_pad(images, mode=\"left-up\"),\n",
        "                    self.crop_shift_pad(images, mode=\"left-down\"),\n",
        "                    self.crop_shift_pad(images, mode=\"right-up\"),\n",
        "                    self.crop_shift_pad(images, mode=\"right-down\"),\n",
        "                ],\n",
        "                axis=-1,\n",
        "            )\n",
        "        # Patchify the images and flatten it\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        flat_patches = self.flatten_patches(patches)\n",
        "        if not self.vanilla:\n",
        "            # Layer normalize the flat patches and linearly project it\n",
        "            tokens = self.layer_norm(flat_patches)\n",
        "            tokens = self.projection(tokens)\n",
        "        else:\n",
        "            # Linearly project the flat patches\n",
        "            tokens = self.projection(flat_patches)\n",
        "        return (tokens, patches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix20SVk5L6Qm"
      },
      "source": [
        "### Visualize the patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCkS6n_6L7AM"
      },
      "outputs": [],
      "source": [
        "# Get a random image from the training dataset and resize the image\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]),\n",
        "    size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        ")\n",
        "\n",
        "# Vanilla patch maker: This takes an image and divides into patches as in the original ViT paper\n",
        "(token, patch) = ShiftedPatchTokenization(vanilla=True)(resized_image / 255.0)\n",
        "(token, patch) = (token[0], patch[0])\n",
        "n = patch.shape[0]\n",
        "count = 1\n",
        "plt.figure(figsize=(4, 4))\n",
        "for row in range(n):\n",
        "  for col in range(n):\n",
        "    plt.subplot(n, n, count)\n",
        "    count = count + 1\n",
        "    image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 3))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Shifted Patch Tokenization: This layer takes the image, shifts it diagonlly and then extracts patches from the concatinated images\n",
        "(token, patch) = ShiftedPatchTokenization(vanilla=False)(resized_image / 255.0)\n",
        "(token, patch) = (token[0], patch[0])\n",
        "n = patch.shape[0]\n",
        "shifted_images = [\"ORIGINAL\", \"LEFT-UP\", \"LEFT-DOWN\", \"RIGHT-UP\", \"RIGHT-DOWN\"]\n",
        "for index, name in enumerate(shifted_images):\n",
        "    print(name)\n",
        "    count = 1\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    for row in range(n):\n",
        "        for col in range(n):\n",
        "            plt.subplot(n, n, count)\n",
        "            count = count + 1\n",
        "            image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 5 * 3))\n",
        "            plt.imshow(image[..., 3 * index : 3 * index + 3])\n",
        "            plt.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGA6hvY-SFrR"
      },
      "source": [
        "## Implement the patch encoding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CHHGS7nSHHQ"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_patches=NUM_PATCHES,\n",
        "      projection_dim=PROJECTION_DIM,\n",
        "      **kwargs,\n",
        "  ):\n",
        "      super().__init__(**kwargs)\n",
        "      self.num_patches = num_patches\n",
        "      self.position_embedding = layers.Embedding(\n",
        "          input_dim=num_patches, output_dim=projection_dim\n",
        "      )\n",
        "      self.positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "\n",
        "  def call(self, encoded_patches):\n",
        "    encoded_positions = self.position_embedding(self.positions)\n",
        "    encoded_patches = encoded_patches + encoded_positions\n",
        "    return encoded_patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11iP_w0dZL1H"
      },
      "source": [
        "### Implementation of Locality Self Attention (LSA)\n",
        "\n",
        "Locality Self Attention (LSA) enhances the regular attention mechanism by addressing the limitations of self-attention, specifically its tendency to prioritize self-token relations over inter-token relations. This adjustment is critical for Vision Transformers (ViTs) dealing with small-sized datasets, as it ensures more balanced and meaningful attention across tokens.\n",
        "\n",
        "### Regular Attention Mechanism\n",
        "\n",
        "The regular attention mechanism can be summarized as follows:\n",
        "\n",
        "1. **Input Components:**\n",
        "   - **Query (Q), Key (K), and Value (V)** are derived from the input.\n",
        "   - In self-attention, Q, K, and V are identical.\n",
        "\n",
        "2. **Compute Similarity:**\n",
        "   - Compute the dot product of the query and key: \\( QK^T \\).\n",
        "   - Scale the result by the square root of the key dimension: \\( QK^T \\) /\\( sqrt(d_k) ).\n",
        "\n",
        "![](https://miro.medium.com/max/396/1*P9sV1xXM10t943bXy_G9yg.png)\n",
        "\n",
        "3. **Apply Softmax:**\n",
        "   - Apply the softmax function to the scaled dot product to obtain attention weights.\n",
        "   - Softmax ensures the weights sum to 1, highlighting the most relevant tokens.\n",
        "\n",
        "4. **Modulate Values:**\n",
        "   - Multiply the attention weights by the value to get the final output.\n",
        "\n",
        "### Enhancements in Locality Self Attention\n",
        "\n",
        "1. **Diagonal Masking:**\n",
        "   - Mask the diagonal of the dot product to prevent self-attention from prioritizing self-token relations.\n",
        "   - This enforces the model to focus on inter-token relationships.\n",
        "\n",
        "2. **Learnable Temperature Term:**\n",
        "   - Introduce a trainable scaling factor (temperature) instead of a fixed constant.\n",
        "   - This allows dynamic modulation of the softmax function based on the data.\n",
        "\n",
        "![](https://i.imgur.com/GTV99pk.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5sxQfbxPaaF"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # The trainable temperature term. The initial value is\n",
        "        # the square root of the key dimension.\n",
        "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
        "\n",
        "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
        "        query = tf.multiply(query, 1.0 / self.tau)\n",
        "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
        "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
        "        attention_scores_dropout = self._dropout_layer(\n",
        "            attention_scores, training=training\n",
        "        )\n",
        "        attention_output = tf.einsum(\n",
        "            self._combine_equation, attention_scores_dropout, value\n",
        "        )\n",
        "        return attention_output, attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZsNReWidsR2"
      },
      "source": [
        "## Implement the MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4N6YyUbL3dJ"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "  for units in hidden_units:\n",
        "    x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "  return x\n",
        "\n",
        "# Build the diagonal attention mask\n",
        "diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
        "diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jULnmen4ehUI"
      },
      "source": [
        "## Build the ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syV51Q7reWun"
      },
      "outputs": [],
      "source": [
        "def create_vit_classifier(vanilla=False):\n",
        "    inputs = layers.Input(shape=INPUT_SHAPE)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder()(tokens)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(TRANSFORMER_LAYERS):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        if not vanilla:\n",
        "            attention_output = MultiHeadAttentionLSA(\n",
        "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
        "            )(x1, x1, attention_mask=diag_attn_mask)\n",
        "        else:\n",
        "            attention_output = layers.MultiHeadAttention(\n",
        "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
        "            )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(NUM_CLASSES)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPS-JToskIDJ"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO6BVbZOkNav",
        "outputId": "29a76ec3-b85f-4ae3-f912-0bf8f1c42ea2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'shifted_patch_tokenization_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3331s\u001b[0m 19s/step - accuracy: 0.0250 - loss: 4.9203 - top-5-accuracy: 0.0999 - val_accuracy: 0.0962 - val_loss: 4.0275 - val_top-5-accuracy: 0.2830\n",
            "Epoch 2/10\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3337s\u001b[0m 19s/step - accuracy: 0.0731 - loss: 4.1047 - top-5-accuracy: 0.2425 - val_accuracy: 0.1480 - val_loss: 3.6603 - val_top-5-accuracy: 0.3980\n",
            "Epoch 3/10\n",
            "\u001b[1m 53/176\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m36:48\u001b[0m 18s/step - accuracy: 0.0989 - loss: 3.9011 - top-5-accuracy: 0.3038"
          ]
        }
      ],
      "source": [
        "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      learning_rate_base,\n",
        "      total_steps,\n",
        "      warmup_learning_rate,\n",
        "      warmup_steps\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.learning_rate_base = learning_rate_base\n",
        "    self.total_steps = total_steps\n",
        "    self.warmup_learning_rate = warmup_learning_rate\n",
        "    self.warmup_steps = warmup_steps\n",
        "    self.pi = tf.constant(np.pi)\n",
        "\n",
        "  def __call__(self, step):\n",
        "    if self.total_steps < self.warmup_steps:\n",
        "      raise ValueError('Tatal_steps must be larger or equal to warmup_steps')\n",
        "\n",
        "    cos_annealed_lr = tf.cos(\n",
        "        self.pi\n",
        "        * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
        "        / float(self.total_steps - self.warmup_steps)\n",
        "    )\n",
        "    learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
        "\n",
        "    if self.warmup_steps > 0:\n",
        "      if self.learning_rate_base < self.warmup_learning_rate:\n",
        "        raise ValueError(\n",
        "            'Learning_rate_base must be larger or equal to \"warmup_learning_rate\".'\n",
        "        )\n",
        "      slope = (\n",
        "          self.learning_rate_base - self.warmup_learning_rate\n",
        "      ) / self.warmup_steps\n",
        "      warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
        "      learning_rate = tf.where(\n",
        "          step < self.warmup_steps, warmup_rate, learning_rate\n",
        "      )\n",
        "    return tf.where(\n",
        "        step > self.total_steps, 0.0, learning_rate, name='learning_rate'\n",
        "    )\n",
        "\n",
        "def run_experiment(model):\n",
        "  total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)\n",
        "  warmup_epoch_percentage = 0.10\n",
        "  warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
        "  scheduled_lrs = WarmUpCosine(\n",
        "      learning_rate_base=LEARNING_RATE,\n",
        "      total_steps=total_steps,\n",
        "      warmup_learning_rate=0.0,\n",
        "      warmup_steps=warmup_steps,\n",
        "  )\n",
        "\n",
        "  optimizer = keras.optimizers.AdamW(\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      weight_decay=WEIGHT_DECAY\n",
        "      )\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[\n",
        "          keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
        "          keras.metrics.SparseTopKCategoricalAccuracy(5, name='top-5-accuracy'),\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  history = model.fit(\n",
        "      x=x_train,\n",
        "      y=y_train,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      epochs=EPOCHS,\n",
        "      validation_split=0.1,\n",
        "  )\n",
        "\n",
        "  _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
        "  print(f'Test accuracy: {round(accuracy * 100, 2)}%')\n",
        "  print(f'Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%')\n",
        "\n",
        "  return history\n",
        "\n",
        "# Run experiments with the vanilla ViT\n",
        "vit = create_vit_classifier(vanilla=True)\n",
        "history = run_experiment(vit)\n",
        "\n",
        "# Run experiments with the Shifted Patch Tokenization and locality Self Attention modified ViT\n",
        "vit_sl = create_vit_classifier(vanilla=False)\n",
        "history = run_experiment(vit_sl)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}