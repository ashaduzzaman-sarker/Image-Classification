{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation and fine-tuning of GCViT: Global Context Vision Transformer for image classification.\n",
        "\n",
        "**Author:** [Ashaduzzaman Sarker](https://github.com/ashaduzzaman-sarker/)\n",
        "<br>\n",
        "**Date created:** 01/07/2024\n",
        "**Reference:**\n",
        "\n",
        " - [Global Context Vision Transformers](\n",
        "https://doi.org/10.48550/arXiv.2206.09959)\n",
        "\n",
        " - [Keras](https://keras.io/examples/vision/image_classification_using_global_context_vision_transformer/)"
      ],
      "metadata": {
        "id": "PyDWTwr9Q-qR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "- This notebook will implement the GCViT (Global Context Vision Transformer) paper presented at ICML 2023 by A Hatamizadeh et al. using multi-backend Keras 3.0.\n",
        "- We will fine-tune the model on the Flower dataset for an image classification task, utilizing official ImageNet pre-trained weights.\n",
        "- A key feature of this notebook is its compatibility with multiple backends: TensorFlow, PyTorch, and JAX, highlighting the true potential of multi-backend Keras."
      ],
      "metadata": {
        "id": "0QrWXkLkSG8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "zktNIXAxhUHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras_cv tensorflow\n",
        "!pip install --upgrade keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZJq0Ix9hVK5",
        "outputId": "0ccbcfd3-d8df-4ce2-8ef7-99d6110c5ada"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_cv\n",
            "  Downloading keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_cv) (24.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras_cv) (2024.5.15)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras_cv) (4.9.6)\n",
            "Collecting keras-core (from keras_cv)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras_cv) (0.2.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tensorflow)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow)\n",
            "  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_cv) (4.66.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_cv) (0.1.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (8.1.7)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (4.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (14.0.2)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (0.1.5)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (1.15.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (0.5.1)\n",
            "Requirement already satisfied: etils[enp,epath,epy,etree]>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (1.7.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets->keras_cv) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets->keras_cv) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets->keras_cv) (3.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: docstring-parser~=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->keras_cv) (0.16)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras-core, keras, tensorflow, keras_cv\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.11.0 keras-3.4.1 keras-core-0.1.7 keras_cv-0.9.0 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.2\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.3.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras_cv.layers import DropPath\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from skimage.data import chelsea\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0BoSaxdthiHk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivation\n",
        "\n",
        "- **Note**: In this section, we will explore the background of GCViT and understand the rationale behind its proposal.\n",
        "- **Transformers in NLP**: Recently, Transformers have become dominant in Natural Language Processing (NLP) tasks due to their self-attention mechanism, which captures both long and short-range information.\n",
        "- **Vision Transformer (ViT)**: Inspired by this trend, Vision Transformer (ViT) proposed using image patches as tokens in a large architecture similar to the original Transformer's encoder.\n",
        "- **ViT vs. CNN**: Despite the historical dominance of Convolutional Neural Networks (CNN) in computer vision, ViT-based models have demonstrated state-of-the-art (SOTA) or competitive performance in various computer vision tasks.\n",
        "<br>\n",
        "\n",
        "![](https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/vit_gif.gif)\n",
        "\n",
        "<br>\n",
        "\n",
        "- The quadratic [O(n^2)] computational complexity of self-attention and the lack of multi-scale information hinder ViT's suitability as a general-purpose architecture for computer vision tasks like segmentation and object detection, which require dense pixel-level predictions.\n",
        "- The Swin Transformer addresses some of ViT's issues by introducing multi-resolution/hierarchical architectures where self-attention is computed in local windows, and cross-window connections like window shifting model interactions across regions.\n",
        "- However, the limited receptive field of local windows in the Swin Transformer fails to capture long-range information, and cross-window connection schemes like window shifting only cover small neighborhoods near each window.\n",
        "- Additionally, the Swin Transformer lacks inductive bias, which encourages translation invariance, a desirable property for general-purpose visual modeling, especially for dense prediction tasks like object detection and semantic segmentation.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/swin_vs_vit.JPG)\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/shifted_window.JPG)\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/swin_arch.JPG)\n",
        "\n",
        "<br>\n",
        "\n",
        "- To address above limitations, **Global Context (GC) ViT** network is proposed."
      ],
      "metadata": {
        "id": "F7hTlIUQSSY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture\n",
        "\n",
        "Let's have a quick overview of our key components:\n",
        "\n",
        "1. **Stem/PatchEmbed**:\n",
        "    - Processes images at the network’s beginning.\n",
        "    - Creates patches/tokens and converts them into embeddings.\n",
        "\n",
        "2. **Level**:\n",
        "    - Repetitive building block that extracts features using different blocks.\n",
        "\n",
        "3. **Global Token Generation/Feature Extraction**:\n",
        "    - Generates global tokens/patches using Depthwise-CNN, SqueezeAndExcitation (Squeeze-Excitation), CNN, and MaxPooling.\n",
        "    - Essentially acts as a feature extractor.\n",
        "\n",
        "4. **Block**:\n",
        "    - Repetitive module that applies attention to the features and projects them to a certain dimension.\n",
        "        1. **Local-MSA**: Local Multi-Head Self-Attention.\n",
        "        2. **Global-MSA**: Global Multi-Head Self-Attention.\n",
        "        3. **MLP**: Linear layer that projects a vector to another dimension.\n",
        "\n",
        "5. **Downsample/ReduceSize**:\n",
        "    - Similar to the Global Token Generation module but uses CNN instead of MaxPooling to downsample, with additional Layer Normalization modules.\n",
        "\n",
        "6. **Head**:\n",
        "    - Responsible for the classification task.\n",
        "        1. **Pooling**: Converts N x 2D features to N x 1D features.\n",
        "        2. **Classifier**: Processes N x 1D features to make a decision about class.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/arch_annot.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "u31qLgiwUYth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unit Blocks\n",
        "\n",
        "**1. SqueezeAndExcitation:**\n",
        "\n",
        "- **Squeeze-Excitation (SE)**, also known as the Bottleneck module, functions as a form of channel attention.\n",
        "- It consists of the following components:\n",
        "    - **AvgPooling**: Averages the spatial dimensions of the input.\n",
        "    - **Dense/FullyConnected (FC)/Linear**: Applies a fully connected layer to the pooled output.\n",
        "    - **GELU**: Uses the Gaussian Error Linear Unit activation function.\n",
        "    - **Sigmoid**: Applies the sigmoid activation function to produce the final output.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/se_annot.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "**2. Fused-MBConv:**\n",
        "\n",
        "- **Fused-MBConv**: Similar to the one used in EfficientNetV2.\n",
        "    - Utilizes **Depthwise-Conv** for depthwise convolution.\n",
        "    - **GELU** for activation.\n",
        "    - **SqueezeAndExcitation** for channel attention.\n",
        "    - **Conv** for regular convolution.\n",
        "    - Includes a **residual connection** to retain input information.\n",
        "- Note: No new modules are declared for this; the corresponding existing modules are applied directly.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/fmb_annot.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "**3. ReduceSize:**\n",
        "\n",
        "- **ReduceSize**: A CNN-based downsample module, referred to as the downsample module in the paper/figure.\n",
        "    - **Fused-MBConv**: Extracts features.\n",
        "    - **Strided Conv**: Simultaneously reduces spatial dimensions and increases channel-wise dimensions of the features.\n",
        "    - **LayerNormalization**: Normalizes features.\n",
        "- Noteworthy: SwinTransformer uses the PatchMerging module instead of ReduceSize, which employs fully-connected/dense/linear modules.\n",
        "- According to the GCViT paper, the purpose of using ReduceSize is to introduce inductive bias through the CNN module.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/down_annot.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "**4. MLP**\n",
        "\n",
        "- **MLP (Multi-Layer Perceptron)**:\n",
        "    - A feed-forward/fully-connected/linear module.\n",
        "    - Projects input to an arbitrary dimension.\n",
        "\n"
      ],
      "metadata": {
        "id": "FzqnaMsFU0q_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HaZv6Sw_Q5S9"
      },
      "outputs": [],
      "source": [
        "# Squeeze and excitation block\n",
        "class SqueezeAndExcitation(layers.Layer):\n",
        "  '''\n",
        "  Args:\n",
        "      output_dim: output features dimension, if 'None' use same dim as input\n",
        "      expansion: expansion ratio\n",
        "  '''\n",
        "\n",
        "  def __init__(self, output_dim=None, expansion=0.25, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.expansion = expansion\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    inp = input_shape[-1]\n",
        "    self.output_dim = self.output_dim or inp\n",
        "    self.avg_pool = layers.GlobalAveragePooling2D(keepdims=True, name='avg_pool')\n",
        "    self.fc = [\n",
        "        layers.Dense(int(inp * self.expansion), use_bias=False, name='fc_0'),\n",
        "        layers.Activation('gelu', name='fc_1'),\n",
        "        layers.Dense(self.output_dim, use_bias=False, name='fc_2'),\n",
        "        layers.Activation('sigmoid', name='fc_3'),\n",
        "    ]\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "    x = self.avg_pool(inputs)\n",
        "    for layer in self.fc:\n",
        "      x = layer(x)\n",
        "    return x * inputs\n",
        "\n",
        "# Down-sampling block\n",
        "class ReduceSize(layers.Layer):\n",
        "  '''\n",
        "  Args:\n",
        "      keepdims: if False spatial dim is reduced and channel dim is increased\n",
        "  '''\n",
        "\n",
        "  def __init__(self, keepdims=False, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.keepdims = keepdims\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    embed_dim = input_shape[-1]\n",
        "    dim_out = embed_dim if self.keepdims else 2 * embed_dim\n",
        "    self.pad_1 = layers.ZeroPadding2D(1, name='pad1')\n",
        "    self.pad_2 = layers.ZeroPadding2D(1, name='pad2')\n",
        "    self.conv = [\n",
        "        layers.DepthwiseConv2D(\n",
        "            kernel_size=3, strides=1, padding='valid', use_bias=False, name='conv_0'\n",
        "        ),\n",
        "        layers.Activation('gelu', name='conv_1'),\n",
        "        SqueezeAndExcitation(name='conv_2'),\n",
        "        layers.Conv2D(\n",
        "            embed_dim,\n",
        "            kernel_size=1,\n",
        "            strides=1,\n",
        "            padding='valid',\n",
        "            use_bias=False,\n",
        "            name='conv_3',\n",
        "        ),\n",
        "    ]\n",
        "    self.reduction = layers.Conv2D(\n",
        "        dim_out,\n",
        "        kernel_size=3,\n",
        "        strides=2,\n",
        "        padding='valid',\n",
        "        use_bias=False,\n",
        "         name='reduction',\n",
        "    )\n",
        "    self.norm1 = layers.LayerNormalization(-1, 1e-05, name='norm1')\n",
        "    self.norm2 = layers.LayerNormalization(-1, 1e-05, name='norm2')\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "    x = self.norm1(inputs)\n",
        "    xr = self.pad1(x)\n",
        "    for layer in self.conv:\n",
        "      xr = layer(xr)\n",
        "    x = x + xr\n",
        "    x = self.pad2(x)\n",
        "    x = self.reduction(x)\n",
        "    x = self.norm2(x)\n",
        "    return x\n",
        "\n",
        "# Multi-Layer Perceptron (MLP) block\n",
        "class MLP(layers.Layer):\n",
        "  '''\n",
        "  Args:\n",
        "      hidden_features=None,\n",
        "      out_features=None,\n",
        "      activation='gelu',\n",
        "      dropout=0.0,\n",
        "      **kwargs,\n",
        "  '''\n",
        "  def __init__(\n",
        "      self,\n",
        "      hidden_features=None,\n",
        "      out_features=None,\n",
        "      activation='gelu',\n",
        "      dropout=0.0,\n",
        "      **kwargs,\n",
        "  ):\n",
        "      super().__init__(**kwargs)\n",
        "      self.hidden_features = hidden_features\n",
        "      self.out_features = out_features\n",
        "      self.activation = activation\n",
        "      self.dropout = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.in_features = input_shape[-1]\n",
        "    self.hidden_features = self.hidden_features or self.in_features\n",
        "    self.out_features = self.out_features or self.in_features\n",
        "    self.fc1 = layers.Dense(self.hidden_features, name='fc1')\n",
        "    self.act = layers.Activation(self.activation, name='act')\n",
        "    self.fc2 = layers.Dense(self.out_features, name='fc2')\n",
        "    self.drop1 = layers.Dropout(self.dropout, name='drop1')\n",
        "    self.drop2 = layers.Dropout(self.dropout, name='drop2')\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "    x = self.fc1(inputs)\n",
        "    x = self.act(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.drop2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stem\n",
        "\n",
        "**Notes**: In the code, this module is referred to as PatchEmbed, but in the paper, it is called Stem.\n",
        "\n",
        "- **PatchEmbed Module**:\n",
        "    - **Padding**: The module first pads the input.\n",
        "    - **Convolutions**: Uses convolutions to extract patches with embeddings.\n",
        "    - **ReduceSize Module**: Utilizes this module to extract features with convolution, without reducing or increasing the spatial dimension.\n",
        "    - **Overlapping Patches**: Unlike ViT or SwinTransformer, GCViT creates overlapping patches. This is indicated by `Conv2D(self.embed_dim, kernel_size=3, strides=2, name='proj')`. Non-overlapping patches would have used the same kernel_size and stride.\n",
        "    - **Spatial Dimension Reduction**: This module reduces the spatial dimension of the input by 4x.\n",
        "\n",
        "**Summary**:\n",
        "\n",
        "image → padding → convolution → (feature extraction + downsample)"
      ],
      "metadata": {
        "id": "G51OhkF2WaQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch embedding block\n",
        "class PatchEmbed(layers.Layer):\n",
        "  '''\n",
        "  Args:\n",
        "      embed_dim: output features dimension\n",
        "  '''\n",
        "\n",
        "  def __init__(self, embed_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.pad = layers.ZeroPadding2D(1, name='pad')\n",
        "    self.proj = layers.Conv2D(self.embed_dim, 3, 2, name='proj')\n",
        "    self.conv_down = ReduceSize(keepdims=True, name='conv_down')\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "    x = self.pad(inputs)\n",
        "    x = self.proj(x)\n",
        "    x = self.conv_down(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "y6B-KG1ysCIE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global Token Gen.\n",
        "\n",
        "**Notes**: This is one of the two CNN modules used to impose inductive bias.\n",
        "\n",
        "- **Global Token Gen./Feature Extraction**:\n",
        "    - **Purpose**: In the level, this module is used to convert the input into global tokens for global-context-attention.\n",
        "    - **Repetition**: According to the paper, this module should be repeated K times, where \\( K = \\log_2(\\frac{H}{h}) \\). Here, \\( H \\) and \\( W \\) are the height and width of the feature map, and \\( h \\) and \\( w \\) are the reduced dimensions.\n",
        "    - **FeatureExtraction**: Similar to the ReduceSize module but with key differences:\n",
        "        - **MaxPooling**: Used to reduce the spatial dimensions.\n",
        "        - **No Channel Increase**: Does not increase the feature dimension (channel-wise).\n",
        "        - **No LayerNormalization**: Does not use LayerNormalization.\n",
        "    - **Global Tokens**: Shared across the entire image, using only one global window for all local tokens in an image, making computation efficient.\n",
        "    - **Shape Transformation**:\n",
        "        - For input feature map with shape (B, H, W, C), the output shape will be (B, h, w, C).\n",
        "        - If global tokens are copied for a total of M local windows in an image, where \\( M = \\frac{H \\times W}{h \\times w} \\) (num_window), the output shape will be (B * M, h, w, C).\n",
        "\n",
        "**Summary**: This module resizes the image to fit the window, creating global tokens for efficient computation.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/global_token_annot.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "DsQ_JpUetXod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction Block\n",
        "class FeatureExtraction(layers.Layer):\n",
        "  '''\n",
        "  Args:\n",
        "      Keepdims: bool argument for maintaining the resolution\n",
        "  '''\n",
        "  def __init__(self, keepdims=False, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.keepdims = keepdims\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    embed_dim = input_shape[-1]\n",
        "    self.pad1 = layers.ZeroPadding2D(1, name='pad1')\n",
        "    self.pad2 = layers.ZeroPadding2D(1, name='pad2')\n",
        "    self.conv = [\n",
        "        layers.DepthwiseConv2D(3, 1, use_bias=False, name='conv_0'),\n",
        "        layers.Activation('gelu', name='conv_1'),\n",
        "        SqueezeAndExcitation(name='conv_2'),\n",
        "        layers.Conv2D(embed_dim, 1, 1, use_bias=False, name='conv_3'),\n",
        "    ]\n",
        "    if not self.keepdims:\n",
        "      self.pool = layers.MaxPool2D(3, 2, name='pool')\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "    x = inputs\n",
        "    xr = self.pad1(x)\n",
        "    for layer in self.conv:\n",
        "      xr = layer(xr)\n",
        "    x = x + xr\n",
        "    if not self.keepdims:\n",
        "      x = self.pool(self.pad2(x))\n",
        "    return x\n",
        "\n",
        "# Global query generator\n",
        "class GlobalQueryGenerator(layers.Layer):\n",
        "  '''\n",
        "  Args:\n",
        "    keepdims: to keep the dimension of FeatureExtraction Layer.\n",
        "      For instance, repeating log(56/7) = 3 blocks, with input\n",
        "      window dimension 56 and output window dimension 7 at down-sampling\n",
        "      ratio 2.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, keepdims=False, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.keepdims = keepdims\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.to_q_global = [\n",
        "        FeatureExtraction(keepdims, name=f'to_q_global_{i}')\n",
        "        for i, keepdims in enumerate(self.keepdims)\n",
        "    ]\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "    x = inputs\n",
        "    for layer in self.to_q_global:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FPKyBXzAtUje"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention\n",
        "\n",
        "**Notes**: This is the core contribution of the paper.\n",
        "\n",
        "- **WindowAttention Module**:\n",
        "    - **Local and Global Attention**: Applies either local or global window attention depending on the `global_query` parameter.\n",
        "    - **Query, Key, Value Creation**:\n",
        "        1. Converts input features into query, key, and value for local attention.\n",
        "        2. Converts input features into key and value for global attention.\n",
        "        3. Global query is taken from the Global Token Gen.\n",
        "\n",
        "![](https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/lvg_msa.PNG)\n",
        "\n",
        "  - **Computation Reduction**: Features (embed_dim) are divided among all heads of the Transformer to reduce computation.\n",
        "       \n",
        "  - **Global Token Process**:\n",
        "      - Global tokens are copied for all local windows to increase efficiency.\n",
        "      - Here, `B_//B` means the number of windows in an image.\n",
        "  - **Attention Application**: Applies either local-window-self-attention or global-window-attention based on the `global_query` parameter.\n",
        "\n",
        "![](https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/lvg_arch.PNG)\n",
        "\n",
        "  - **Relative Positional Embedding**: Adds relative-positional-embedding with the attention mask instead of the patch embedding.\n",
        "        \n",
        "\n",
        "**Explanation**:\n",
        "- **Local Attention**: The query is local, limited to the local window (red square border), hence no access to long-range information.\n",
        "- **Global Attention**: With the global query, it is not limited to local windows (blue square border), allowing access to long-range information.\n",
        "- **Comparative Attention**:\n",
        "    - **ViT**: Compares image-tokens with image-tokens.\n",
        "    - **SwinTransformer**: Compares window-tokens with window-tokens.\n",
        "    - **GCViT**: Compares image-tokens with window-tokens by resizing image-tokens to fit window-tokens using the Global Token Gen./FeatureExtraction CNN module.\n",
        "\n",
        "**Comparison Table**:\n",
        "\n",
        "| Model             | Query Tokens     | Key-Value Tokens | Attention Type       | Attention Coverage |\n",
        "|-------------------|------------------|------------------|----------------------|--------------------|\n",
        "| **ViT**           | image            | image            | self-attention       | global             |\n",
        "| **SwinTransformer**| window           | window           | self-attention       | local              |\n",
        "| **GCViT**         | resized-image    | window           | image-window attention | global             |\n",
        "\n",
        "**Summary**:\n",
        "- The Global Token Gen./FeatureExtraction CNN module resizes image-tokens to fit window-tokens, enabling GCViT to perform efficient global attention by comparing resized image-tokens with window-tokens."
      ],
      "metadata": {
        "id": "0YacSqRw2kN4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1ntrfuC2l0R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}