{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPX01NQ0wy9HKLyL1d/EgK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashaduzzaman-sarker/Computer-Vision-Projects/blob/main/Image_classification_with_modern_MLP(MLP_Mixer%2C_FNet%2C_and_gMLP)_models_for_CIFAR_100_using_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image classification with modern MLP(MLP-Mixer, FNet, and gMLP) models for CIFAR-100 using Keras\n",
        "\n",
        "**Author:** [Ashaduzzaman Piash](https://github.com/ashaduzzaman-sarker/)\n",
        "<br>\n",
        "**Date created:** 2024/06/17"
      ],
      "metadata": {
        "id": "deG66AiY4qdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "### Overview\n",
        "This project demonstrates the implementation of three modern attention-free, multi-layer perceptron (MLP) based models for image classification, using the CIFAR-100 dataset:\n",
        "\n",
        "- **[MLP-Mixer Model](\n",
        "https://doi.org/10.48550/arXiv.2105.01601)**: Developed by Ilya Tolstikhin et al., this model utilizes two types of MLPs to process images.\n",
        "- **[FNet Model](\n",
        "https://doi.org/10.48550/arXiv.2105.03824\n",
        ")**: Proposed by James Lee-Thorp et al., it relies on the unparameterized Fourier Transform for feature extraction.\n",
        "- **[gMLP Model](\n",
        "https://doi.org/10.48550/arXiv.2105.08050\n",
        ")**: Created by Hanxiao Liu et al., this model incorporates gating mechanisms within MLPs.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "The goal of this example is not to compare the performance of these models, as their effectiveness can vary across different datasets and with optimized hyperparameters. Instead, the focus is on providing simple implementations of their core components.\n",
        "\n",
        "### Key Points\n",
        "\n",
        "- **MLP-Mixer**: Combines two types of MLPs for spatial and channel-wise mixing of image data.\n",
        "- **FNet**: Uses Fourier Transforms instead of attention mechanisms for efficient feature extraction.\n",
        "- **gMLP**: Incorporates gating mechanisms to improve the expressiveness of standard MLPs.\n",
        "\n",
        "### Implementation Focus\n",
        "\n",
        "- Demonstrates the main building blocks of each model.\n",
        "- Uses the CIFAR-100 dataset for image classification tasks.\n",
        "- Provides insights into attention-free neural network architectures for image processing."
      ],
      "metadata": {
        "id": "DSPQRJE9mEdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "PBC5SZoandwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update Keras 3\n",
        "!pip install --upgrade keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9-CtQ1XVDcIb",
        "outputId": "f09de059-aa57-4c1e-e2cf-d435979302fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.3.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NImayZCl4mS2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data"
      ],
      "metadata": {
        "id": "JqQo3I_MnmaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7NKXEUAnoYG",
        "outputId": "0505fef1-b6db-4d74-bd23-802bbad0a6ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure the hyperparameters"
      ],
      "metadata": {
        "id": "PN1fMuRZoj0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 10  # Recommended num_epochs = 50\n",
        "dropout_rate = 0.2\n",
        "image_size = 64  # We'll resize input images to this size.\n",
        "patch_size = 8  # Size of the patches to be extracted from the input images.\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
        "embedding_dim = 256  # Number of hidden units.\n",
        "num_blocks = 4  # Number of blocks.\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHNFmxcXoeNG",
        "outputId": "90be928c-6553-49f5-e40a-ee78f646f5a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 8 X 8 = 64 \n",
            "Patches per image: 64\n",
            "Elements per patch (3 channels): 192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a classification model"
      ],
      "metadata": {
        "id": "GlC6Z35fqwqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier(blocks, positional_encoding=False):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
        "    x = layers.Dense(units=embedding_dim)(patches)\n",
        "    if positional_encoding:\n",
        "        x = x + PositionEmbedding(sequence_length=num_patches)(x)\n",
        "    # Process x using the module blocks.\n",
        "    x = blocks(x)\n",
        "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n",
        "    representation = layers.GlobalAveragePooling1D()(x)\n",
        "    # Apply dropout.\n",
        "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
        "    # Compute logits outputs.\n",
        "    logits = layers.Dense(num_classes)(representation)\n",
        "    # Create the Keras model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits)"
      ],
      "metadata": {
        "id": "zXBBGhK7qqF0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define an experiment"
      ],
      "metadata": {
        "id": "_hYG1c1HtlCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model):\n",
        "    # Create Adam optimizer with weight decay.\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "    )\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=5\n",
        "    )\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history"
      ],
      "metadata": {
        "id": "OHIzqpbwtdQt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use data augmentation"
      ],
      "metadata": {
        "id": "NeLzXsAgxHr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "metadata": {
        "id": "32tTF_iTxIfh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement patch extraction as a layer"
      ],
      "metadata": {
        "id": "GePy1BAUzill"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, x):\n",
        "        patches = keras.ops.image.extract_patches(x, self.patch_size)\n",
        "        batch_size = keras.ops.shape(patches)[0]\n",
        "        num_patches = keras.ops.shape(patches)[1] * keras.ops.shape(patches)[2]\n",
        "        patch_dim = keras.ops.shape(patches)[3]\n",
        "        out = keras.ops.reshape(patches, (batch_size, num_patches, patch_dim))\n",
        "        return out"
      ],
      "metadata": {
        "id": "gsorTBBMzf7m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement position embedding as a layer"
      ],
      "metadata": {
        "id": "vfMqvYXu01cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbedding(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sequence_length,\n",
        "        initializer=\"glorot_uniform\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        if sequence_length is None:\n",
        "            raise ValueError(\"`sequence_length` must be an Integer, received `None`.\")\n",
        "        self.sequence_length = int(sequence_length)\n",
        "        self.initializer = keras.initializers.get(initializer)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"sequence_length\": self.sequence_length,\n",
        "                \"initializer\": keras.initializers.serialize(self.initializer),\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        feature_size = input_shape[-1]\n",
        "        self.position_embeddings = self.add_weight(\n",
        "            name=\"embeddings\",\n",
        "            shape=[self.sequence_length, feature_size],\n",
        "            initializer=self.initializer,\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, start_index=0):\n",
        "        shape = keras.ops.shape(inputs)\n",
        "        feature_length = shape[-1]\n",
        "        sequence_length = shape[-2]\n",
        "        # trim to match the length of the input sequence, which might be less\n",
        "        # than the sequence_length of the layer.\n",
        "        position_embeddings = keras.ops.convert_to_tensor(self.position_embeddings)\n",
        "        position_embeddings = keras.ops.slice(\n",
        "            position_embeddings,\n",
        "            (start_index, 0),\n",
        "            (sequence_length, feature_length),\n",
        "        )\n",
        "        return keras.ops.broadcast_to(position_embeddings, shape)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "metadata": {
        "id": "DV7CttWx0tr6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The MLP-Mixer Model\n",
        "\n",
        "The MLP-Mixer is a novel neural network architecture that relies exclusively on multi-layer perceptrons (MLPs). It uses two types of MLP layers to process image data:\n",
        "\n",
        "1. **Patch-wise MLPs**: Applied independently to image patches, these layers mix the per-location features.\n",
        "2. **Channel-wise MLPs**: Applied across patches along the channels, these layers mix spatial information.\n",
        "\n",
        "This approach is analogous to depthwise separable convolutions used in models like Xception, but with some key differences:\n",
        "\n",
        "- **Chained Dense Transforms**: Instead of convolutions, the MLP-Mixer chains two dense (fully connected) transformations.\n",
        "- **No Max Pooling**: The model does not use max pooling for down-sampling.\n",
        "- **Layer Normalization**: Replaces batch normalization with layer normalization to stabilize and accelerate training.\n",
        "\n",
        "### Key Points\n",
        "\n",
        "- **Patch-wise MLPs**: Process each patch of the image independently to mix features within the patch.\n",
        "- **Channel-wise MLPs**: Mix features across different patches along the channels to capture spatial relationships.\n",
        "- **Chained Dense Layers**: Utilizes dense layers in place of convolutions to perform feature mixing.\n",
        "- **Layer Normalization**: Ensures stability and faster convergence during training.\n",
        "\n",
        "![](https://camo.githubusercontent.com/20dc92bd1da191765477f203d02535239ffdaad6d12b4fc0a5ddd67b3d3f290e/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f70726f746f6e782d636c6f75642d73746f726167652f43617074757265332e504e47)"
      ],
      "metadata": {
        "id": "0R7Jgn5V4yrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the MLP-Mixer module"
      ],
      "metadata": {
        "id": "WMs15V8i4_X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPMixerLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.mlp1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches, activation=\"gelu\"),\n",
        "                layers.Dense(units=num_patches),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.mlp2 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches, activation=\"gelu\"),\n",
        "                layers.Dense(units=hidden_units),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        return super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize(inputs)\n",
        "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
        "        x_channels = keras.ops.transpose(x, axes=(0, 2, 1))\n",
        "        # Apply mlp1 on each channel independently.\n",
        "        mlp1_outputs = self.mlp1(x_channels)\n",
        "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
        "        mlp1_outputs = keras.ops.transpose(mlp1_outputs, axes=(0, 2, 1))\n",
        "        # Add skip connection.\n",
        "        x = mlp1_outputs + inputs\n",
        "        # Apply layer normalization.\n",
        "        x_patches = self.normalize(x)\n",
        "        # Apply mlp2 on each patch independtenly.\n",
        "        mlp2_outputs = self.mlp2(x_patches)\n",
        "        # Add skip connection.\n",
        "        x = x + mlp2_outputs\n",
        "        return x"
      ],
      "metadata": {
        "id": "bnY3OGRz4ew8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build, train, and evaluate the MLP-Mixer model"
      ],
      "metadata": {
        "id": "aYODOQs6CdCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlpmixer_blocks = keras.Sequential(\n",
        "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.005\n",
        "mlpmixer_classifier = build_classifier(mlpmixer_blocks)\n",
        "history = run_experiment(mlpmixer_classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpcDdxisxFhA",
        "outputId": "11f16e1e-80ad-439b-b1f6-ed6c927b79b1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - acc: 0.3523 - loss: 2.6181 - top5-acc: 0.6757\n",
            "Test accuracy: 35.57%\n",
            "Test top 5 accuracy: 67.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The FNet model\n",
        "\n",
        "The FNet is a neural network architecture inspired by the Transformer block but replaces the self-attention mechanism with a parameter-free 2D Fourier transformation layer. Here's how it works:\n",
        "\n",
        "- **Fourier Transform**: Instead of using self-attention to capture dependencies in the input data, FNet applies the Fourier Transform.\n",
        "  - **1D Fourier Transform Along Patches**: This transform is applied along the sequence of image patches.\n",
        "  - **1D Fourier Transform Along Channels**: This transform is applied along the feature channels of the image.\n",
        "\n",
        "### Key Points\n",
        "\n",
        "- **Transformer-like Structure**: FNet retains a similar structure to the Transformer block but swaps out the self-attention layer for a Fourier Transform layer.\n",
        "- **Parameter-free**: The Fourier Transform layers do not have learnable parameters, simplifying the model.\n",
        "- **Two-dimensional Fourier Transform**: Consists of two 1D Fourier Transforms, one along the patches and one along the channels.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1010/1*7ZfynrPBS6jNIu4U49TMCA.png)"
      ],
      "metadata": {
        "id": "TwfsxPawEwww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the FNet module"
      ],
      "metadata": {
        "id": "CwFmbAD6I06h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FNetLayer(layers.Layer):\n",
        "    def __init__(self, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embedding_dim, activation=\"gelu\"),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply fourier transformations.\n",
        "        real_part = inputs\n",
        "        im_part = keras.ops.zeros_like(inputs)\n",
        "        x = keras.ops.fft2((real_part, im_part))[0]\n",
        "        # Add skip connection.\n",
        "        x = x + inputs\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize1(x)\n",
        "        # Apply Feedfowrad network.\n",
        "        x_ffn = self.ffn(x)\n",
        "        # Add skip connection.\n",
        "        x = x + x_ffn\n",
        "        # Apply layer normalization.\n",
        "        return self.normalize2(x)"
      ],
      "metadata": {
        "id": "XaoPbIBOTZQD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build, train, and evaluate the FNet model\n"
      ],
      "metadata": {
        "id": "4EFmJ6_vV1x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnet_blocks = keras.Sequential(\n",
        "    [FNetLayer(embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.001\n",
        "fnet_classifier = build_classifier(fnet_blocks, positional_encoding=True)\n",
        "history = run_experiment(fnet_classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T32NlTU2I441",
        "outputId": "accefc31-2843-4717-eb3d-77420e395229"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - acc: 0.3834 - loss: 2.3661 - top5-acc: 0.7068\n",
            "Test accuracy: 37.87%\n",
            "Test top 5 accuracy: 70.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The gMLP Model\n",
        "\n",
        "The gMLP (Gated MLP) is a neural network architecture that introduces a Spatial Gating Unit (SGU) to enable interactions across image patches along the spatial dimension (channels). Here's how it works:\n",
        "\n",
        "1. **Spatial Transformation**: The input is transformed by applying a linear projection across patches along the channels.\n",
        "2. **Element-wise Multiplication**: The original input is multiplied element-wise with its spatial transformation.\n",
        "\n",
        "### Key Points\n",
        "\n",
        "- **Spatial Gating Unit (SGU)**: This unit facilitates interactions across patches by transforming the input spatially and then gating it.\n",
        "- **Linear Projection**: A linear transformation is applied to the input across the patches along the channel dimension.\n",
        "- **Element-wise Multiplication**: The transformed input and the original input are combined through element-wise multiplication, enabling the model to learn cross-patch relationships.\n",
        "\n",
        "![](https://production-media.paperswithcode.com/methods/641e1c00-a87b-40ce-a0ab-af50ac6aa318.png\n",
        ")"
      ],
      "metadata": {
        "id": "xWhDdok9WwLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the gMLP module"
      ],
      "metadata": {
        "id": "ZJPedJViXhxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class gMLPLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.channel_projection1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embedding_dim * 2, activation=\"gelu\"),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.channel_projection2 = layers.Dense(units=embedding_dim)\n",
        "\n",
        "        self.spatial_projection = layers.Dense(\n",
        "            units=num_patches, bias_initializer=\"Ones\"\n",
        "        )\n",
        "\n",
        "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def spatial_gating_unit(self, x):\n",
        "        # Split x along the channel dimensions.\n",
        "        # Tensors u and v will in the shape of [batch_size, num_patchs, embedding_dim].\n",
        "        u, v = keras.ops.split(x, indices_or_sections=2, axis=2)\n",
        "        # Apply layer normalization.\n",
        "        v = self.normalize2(v)\n",
        "        # Apply spatial projection.\n",
        "        v_channels = keras.ops.transpose(v, axes=(0, 2, 1))\n",
        "        v_projected = self.spatial_projection(v_channels)\n",
        "        v_projected = keras.ops.transpose(v_projected, axes=(0, 2, 1))\n",
        "        # Apply element-wise multiplication.\n",
        "        return u * v_projected\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize1(inputs)\n",
        "        # Apply the first channel projection. x_projected shape: [batch_size, num_patches, embedding_dim * 2].\n",
        "        x_projected = self.channel_projection1(x)\n",
        "        # Apply the spatial gating unit. x_spatial shape: [batch_size, num_patches, embedding_dim].\n",
        "        x_spatial = self.spatial_gating_unit(x_projected)\n",
        "        # Apply the second channel projection. x_projected shape: [batch_size, num_patches, embedding_dim].\n",
        "        x_projected = self.channel_projection2(x_spatial)\n",
        "        # Add skip connection.\n",
        "        return x + x_projected"
      ],
      "metadata": {
        "id": "bSjyxBcUI0Ty"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build, train, and evaluate the gMLP model"
      ],
      "metadata": {
        "id": "MopiTT6tYFez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gmlp_blocks = keras.Sequential(\n",
        "    [gMLPLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.003\n",
        "gmlp_classifier = build_classifier(gmlp_blocks)\n",
        "history = run_experiment(gmlp_classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTIm7sehYDha",
        "outputId": "c46959bb-1960-4c3c-da41-234d7bfdeab1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - acc: 0.3886 - loss: 2.3994 - top5-acc: 0.6950\n",
            "Test accuracy: 38.79%\n",
            "Test top 5 accuracy: 69.64%\n"
          ]
        }
      ]
    }
  ]
}